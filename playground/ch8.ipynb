{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2d7f04-9c28-46c2-ad07-598b14e6564b",
   "metadata": {},
   "source": [
    "# 8 The Attention Mechanism from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23d4dda-b770-4420-959e-53ccb2472a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:38:58.204429Z",
     "iopub.status.busy": "2023-06-22T04:38:58.203957Z",
     "iopub.status.idle": "2023-06-22T04:38:58.742177Z",
     "shell.execute_reply": "2023-06-22T04:38:58.741862Z",
     "shell.execute_reply.started": "2023-06-22T04:38:58.204404Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5921e-970e-4440-9b1b-0d2a071684b6",
   "metadata": {},
   "source": [
    "## 8.3 The General Attention Mechanism with NumPy and SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd5e08-d7a0-4c61-a2b6-e45dbec4da69",
   "metadata": {},
   "source": [
    "Consider the following four fabricated word embeddings. (In practice, word embeddings are usually the output of an encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a79c745-70c9-473a-a863-444d84f918a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T03:57:08.389163Z",
     "iopub.status.busy": "2023-06-22T03:57:08.388059Z",
     "iopub.status.idle": "2023-06-22T03:57:08.409238Z",
     "shell.execute_reply": "2023-06-22T03:57:08.407182Z",
     "shell.execute_reply.started": "2023-06-22T03:57:08.389115Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e8023-1e49-48fd-b935-d2ab86f8468a",
   "metadata": {},
   "source": [
    "And the following randomly-initiated Query, Key and Value matrices. (Again, in practice, these would be akin to weights learned during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fbd8c1-0a72-4e6f-820a-a687c4343672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:05:20.543177Z",
     "iopub.status.busy": "2023-06-22T04:05:20.542590Z",
     "iopub.status.idle": "2023-06-22T04:05:20.556967Z",
     "shell.execute_reply": "2023-06-22T04:05:20.555681Z",
     "shell.execute_reply.started": "2023-06-22T04:05:20.543138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "W_Q = np.random.randint(3, size=(3, 3))\n",
    "W_K = np.random.randint(3, size=(3, 3))\n",
    "W_V = np.random.randint(3, size=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e51727-603a-43ef-939d-17fe2b097c02",
   "metadata": {},
   "source": [
    "Note how the number of rows of each of these matrices equals the dimensionality of our word embeddings. We will now calculate the query, key and value _vectors_ using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bc6e965-7d6d-419b-80fd-dcd967916607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:44:14.517210Z",
     "iopub.status.busy": "2023-06-22T04:44:14.516813Z",
     "iopub.status.idle": "2023-06-22T04:44:14.534098Z",
     "shell.execute_reply": "2023-06-22T04:44:14.532191Z",
     "shell.execute_reply.started": "2023-06-22T04:44:14.517184Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (3,), (3,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    "\n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    "\n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V\n",
    "\n",
    "query_4 = word_4 @ W_Q\n",
    "key_4 = word_4 @ W_K\n",
    "value_4 = word_4 @ W_V\n",
    "\n",
    "query_1.shape, key_1.shape, value_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d21c9-d30f-49d1-ac8e-9e10114d4083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:08:51.871233Z",
     "iopub.status.busy": "2023-06-22T04:08:51.870012Z",
     "iopub.status.idle": "2023-06-22T04:08:51.894362Z",
     "shell.execute_reply": "2023-06-22T04:08:51.886850Z",
     "shell.execute_reply.started": "2023-06-22T04:08:51.871188Z"
    },
    "tags": []
   },
   "source": [
    "Next, let us calculate the \"alignment scores\" for the first word using its query vector and all the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c0ec16-0853-409c-ba28-35347f5ba534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:43:58.288876Z",
     "iopub.status.busy": "2023-06-22T04:43:58.288117Z",
     "iopub.status.idle": "2023-06-22T04:43:58.301974Z",
     "shell.execute_reply": "2023-06-22T04:43:58.301133Z",
     "shell.execute_reply.started": "2023-06-22T04:43:58.288837Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_1 = np.array(\n",
    "    [\n",
    "        np.dot(query_1, key_1),\n",
    "        np.dot(query_1, key_2),\n",
    "        np.dot(query_1, key_3),\n",
    "        np.dot(query_1, key_4),\n",
    "    ]\n",
    ")\n",
    "scores_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d23b8-aa89-4a31-8d27-bc534b2f0709",
   "metadata": {},
   "source": [
    "Next, we can calculate the \"attention weights\" by applying a softmax function to the scores. But first, it is cusotmary to divide the scores by the square root of the square root of the dimensionality of keys, in order to control its variance and keep the gradients stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d567d715-febf-4792-8894-aaa6457426d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:40:03.860396Z",
     "iopub.status.busy": "2023-06-22T04:40:03.859927Z",
     "iopub.status.idle": "2023-06-22T04:40:03.877023Z",
     "shell.execute_reply": "2023-06-22T04:40:03.874564Z",
     "shell.execute_reply.started": "2023-06-22T04:40:03.860365Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_1 = softmax(scores_1 / np.sqrt(key_1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbb543-7d7c-4e01-971c-bba773732b3d",
   "metadata": {},
   "source": [
    "Finally, we calculate attention as the weighted sum of the four value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5cfe840-a509-40a1-9e98-77e5c22662ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:47:53.729359Z",
     "iopub.status.busy": "2023-06-22T04:47:53.728484Z",
     "iopub.status.idle": "2023-06-22T04:47:53.753234Z",
     "shell.execute_reply": "2023-06-22T04:47:53.752266Z",
     "shell.execute_reply.started": "2023-06-22T04:47:53.729317Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98522025, 1.74174051, 0.75652026])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_1 = (\n",
    "    weights_1[0] * value_1\n",
    "    + weights_1[1] * value_2\n",
    "    + weights_1[2] * value_3\n",
    "    + weights_1[3] * value_4\n",
    ")\n",
    "attention_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9508937-3970-4601-b8fb-4175ebcd5107",
   "metadata": {},
   "source": [
    "Of course, we could do this for all four token embeddings in parallel using matrix algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d44a0f43-833a-4503-86a4-1a03b5c9c592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T04:51:52.579597Z",
     "iopub.status.busy": "2023-06-22T04:51:52.578746Z",
     "iopub.status.idle": "2023-06-22T04:51:52.596256Z",
     "shell.execute_reply": "2023-06-22T04:51:52.595406Z",
     "shell.execute_reply.started": "2023-06-22T04:51:52.579558Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98522025, 1.74174051, 0.75652026],\n",
       "       [0.90965265, 1.40965265, 0.5       ],\n",
       "       [0.99851226, 1.75849334, 0.75998108],\n",
       "       [0.99560386, 1.90407309, 0.90846923]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array([word_1, word_2, word_3, word_4])\n",
    "\n",
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    "\n",
    "scores = Q @ K.T\n",
    "weights = softmax(scores / np.sqrt(K.shape[1]), axis=1)\n",
    "attention = weights @ V\n",
    "\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8cd39-525e-4668-b4b7-1c812a5e44a2",
   "metadata": {},
   "source": [
    "The specifics of what the queries, keys and values are depends on the specific architecture. For instance, in the Bahdanau attention mechanism, the queries would be analogous to the previous decoder output $s_{t-1}$, the keys would be analogous to the encoded inputs (concatenated forward and backward hidden states) $h_{i}$ and the values would be same vectors as the keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392e115-298f-4ac6-908d-109344f5695f",
   "metadata": {},
   "source": [
    "**Note:** This simple attention mechanism has no learnable parameters! All the learning is in the word embeddings. So in the simple attention mechanism, a large part of the behaviour of the model comes from the parameters _upstream_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

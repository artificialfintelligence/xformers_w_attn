{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0969831-66dd-4de9-9938-51a34a33426e",
   "metadata": {},
   "source": [
    "# 17 Implementing the Transformer Encoder in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86875a00-50a6-41a2-969e-225f2ccb23b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T03:31:02.413694Z",
     "iopub.status.busy": "2023-08-29T03:31:02.412747Z",
     "iopub.status.idle": "2023-08-29T03:31:02.424846Z",
     "shell.execute_reply": "2023-08-29T03:31:02.421674Z",
     "shell.execute_reply.started": "2023-08-29T03:31:02.413646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from xformer.multihead_attention import MultiHeadAttention\n",
    "from xformer.positional_encoding import CustomEmbeddingWithFixedPosnWts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1366935-0411-4158-9864-566090cea24a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 17.1 Recap of the Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dd370-8d2b-4743-ac97-a3f8e63b684a",
   "metadata": {},
   "source": [
    "Recall that the encoder block is a stack of N identical layers. Each layer consists of a multi-head self-attention layer which we expatiated on in Ch. 16. Now we will add some further important missing details.  \n",
    "\n",
    "1. The multi-head self-attention is one of _two_ sub-layers in each stack of the encoder. The _other_ sub-layer is a fully-connected feed-forward layer.\n",
    "2. After each of the aforementioned two sub-layers, there's a normalization layer which first adds the sublayer's output to its inputs (this forms what we call a \"residual connection\") and then normalizes the result.\n",
    "3. Regularization is performed by applying a dropout layer to the outputs of each of the aforementioned \"sub-layers\" right before the normalization step, as well as to the positionally-encoded embeddings right before they are fed into the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719f9c9-cef3-4ca4-8369-73a438baf8b4",
   "metadata": {},
   "source": [
    "## 17.2 Implementing the Transformer Encoder from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff86d25-5117-4b99-b9d4-791c7120597e",
   "metadata": {},
   "source": [
    "Note: We will reuse the multi-head attention and the positional embedding logic we implemented in previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160e721-9ef3-4379-a481-4407d8757eeb",
   "metadata": {},
   "source": [
    "### The Feedforward Network and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ab864-0954-4214-8aea-525bae6ee179",
   "metadata": {},
   "source": [
    "In AIAYN this is simply two fully-connected (AKA Linear) layers with a ReLU activation in between. The first FF layer's output has dims $d_{ff}=2048$ and the second one brings it back to $d_{model}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2841d46e-abfa-4834-b7e4-a882f01a8b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T03:03:32.501405Z",
     "iopub.status.busy": "2023-08-30T03:03:32.500941Z",
     "iopub.status.idle": "2023-08-30T03:03:32.513430Z",
     "shell.execute_reply": "2023-08-30T03:03:32.512419Z",
     "shell.execute_reply.started": "2023-08-30T03:03:32.501375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected_1 = Dense(d_ff) # First fully-connected layer\n",
    "        self.fully_connected_2 = Dense(d_model) # Second fully-connected layer\n",
    "        self.activation = ReLU() # ReLU activation layer to come in between\n",
    "        \n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        fc1_output = self.fully_connected1(x)\n",
    "        fc2_output = self.fully_connected2(self.activation(x_fc1))\n",
    "        return fc2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c1404-2bf3-4400-847d-1978aac771a7",
   "metadata": {},
   "source": [
    "Next, we define our \"Layer Normalization\" layer. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf), not to be confused with but in many ways similar to [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf), is a way of ensuring better, more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30501043-59f7-49cc-8bd7-3f23acb17072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T03:03:30.346601Z",
     "iopub.status.busy": "2023-08-30T03:03:30.345830Z",
     "iopub.status.idle": "2023-08-30T03:03:30.367033Z",
     "shell.execute_reply": "2023-08-30T03:03:30.361939Z",
     "shell.execute_reply.started": "2023-08-30T03:03:30.346564Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddAndNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization() # Layer normalization layer\n",
    "        \n",
    "    def call(self, x, sublayer_x):\n",
    "        # Note: The sublayer's input and output need to be of the same shape to be summable\n",
    "        add = x + sublayer_x\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0382-8b56-4919-be50-57260a87ae9d",
   "metadata": {},
   "source": [
    "### The Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fb36b-f609-4190-b2d5-d9ecb86569d2",
   "metadata": {},
   "source": [
    "Next, we will define what an encoder layer looks like. **Note:** I may have used the word \"encoder block\" elsewhere. Going forward, I will try to stay consistent and use \"encoder layer\". Just picture AIAYN's block diagram and recall that they stack N=6 of for these to form their transformer's encoder \"block\". But we'll get to that in the next section.  \n",
    "The `training` flag in the `call()` function is there so that we don't perform dropout regularization during testing and inference.  \n",
    "The `padding_mask` argument, as explained in previous chapters, is to suppress zero padding tokens in input sequences from being processed along with valid input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392efb5a-69f0-484a-b838-909862fb9d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T03:13:14.477171Z",
     "iopub.status.busy": "2023-08-30T03:13:14.476172Z",
     "iopub.status.idle": "2023-08-30T03:13:14.488551Z",
     "shell.execute_reply": "2023-08-30T03:13:14.485513Z",
     "shell.execute_reply.started": "2023-08-30T03:13:14.477127Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.add_norm1 = AddAndNorm()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.add_norm2 = AddAndNorm()\n",
    "    \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c730a-0846-4280-a51a-bcf62cef9f31",
   "metadata": {},
   "source": [
    "### The Transformer Encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

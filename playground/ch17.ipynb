{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0969831-66dd-4de9-9938-51a34a33426e",
   "metadata": {},
   "source": [
    "# 17 Implementing the Transformer Encoder in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86875a00-50a6-41a2-969e-225f2ccb23b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:03.385625Z",
     "iopub.status.busy": "2023-08-31T18:51:03.385413Z",
     "iopub.status.idle": "2023-08-31T18:51:06.487747Z",
     "shell.execute_reply": "2023-08-31T18:51:06.487435Z",
     "shell.execute_reply.started": "2023-08-31T18:51:03.385563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    ReLU,\n",
    ")\n",
    "from numpy import random\n",
    "from xformer.multihead_attention import MultiHeadAttention\n",
    "from xformer.positional_encoding import CustomEmbeddingWithFixedPosnWts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1366935-0411-4158-9864-566090cea24a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 17.1 Recap of the Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dd370-8d2b-4743-ac97-a3f8e63b684a",
   "metadata": {},
   "source": [
    "Recall that the encoder block is a stack of N identical layers. Each layer consists of a multi-head self-attention layer which we expatiated on in Ch. 16. Now we will add some further important missing details.  \n",
    "\n",
    "1. The multi-head self-attention is one of _two_ sub-layers in each stack of the encoder. The _other_ sub-layer is a fully-connected feed-forward layer.\n",
    "2. After each of the aforementioned two sub-layers, there's a normalization layer which first adds the sublayer's output to its inputs (this forms what we call a \"residual connection\") and then normalizes the result.\n",
    "3. Regularization is performed by applying a dropout layer to the outputs of each of the aforementioned \"sub-layers\" right before the normalization step, as well as to the positionally-encoded embeddings right before they are fed into the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719f9c9-cef3-4ca4-8369-73a438baf8b4",
   "metadata": {},
   "source": [
    "## 17.2 Implementing the Transformer Encoder from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff86d25-5117-4b99-b9d4-791c7120597e",
   "metadata": {},
   "source": [
    "Note: We will reuse the multi-head attention and the positional embedding logic we implemented in previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160e721-9ef3-4379-a481-4407d8757eeb",
   "metadata": {},
   "source": [
    "### The Feedforward Network and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ab864-0954-4214-8aea-525bae6ee179",
   "metadata": {},
   "source": [
    "In AIAYN this is simply two fully-connected (AKA Linear) layers with a ReLU activation in between. The first FF layer's output has dims $d_{ff}=2048$ and the second one brings it back to $d_{model}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2841d46e-abfa-4834-b7e4-a882f01a8b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:06.489083Z",
     "iopub.status.busy": "2023-08-31T18:51:06.488859Z",
     "iopub.status.idle": "2023-08-31T18:51:06.491855Z",
     "shell.execute_reply": "2023-08-31T18:51:06.491575Z",
     "shell.execute_reply.started": "2023-08-31T18:51:06.489072Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected_1 = Dense(d_ff)  # First fully-connected layer\n",
    "        self.fully_connected_2 = Dense(d_model)  # Second fully-connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer to come in between\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        fc1_output = self.fully_connected_1(x)\n",
    "        fc2_output = self.fully_connected_2(self.activation(fc1_output))\n",
    "        return fc2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c1404-2bf3-4400-847d-1978aac771a7",
   "metadata": {},
   "source": [
    "Next, we define our \"Layer Normalization\" layer. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf), not to be confused with but in many ways similar to [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf), is a way of ensuring better, more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30501043-59f7-49cc-8bd7-3f23acb17072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:06.492296Z",
     "iopub.status.busy": "2023-08-31T18:51:06.492214Z",
     "iopub.status.idle": "2023-08-31T18:51:06.494353Z",
     "shell.execute_reply": "2023-08-31T18:51:06.494075Z",
     "shell.execute_reply.started": "2023-08-31T18:51:06.492287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddAndNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # Note: The sublayer's input and output need to be of the same shape to be summable\n",
    "        add = x + sublayer_x\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0382-8b56-4919-be50-57260a87ae9d",
   "metadata": {},
   "source": [
    "### The Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fb36b-f609-4190-b2d5-d9ecb86569d2",
   "metadata": {},
   "source": [
    "Next, we will define what an encoder layer looks like. **Note:** I may have used the word \"encoder block\" elsewhere. Going forward, I will try to stay consistent and use \"encoder layer\". Just picture AIAYN's block diagram and recall that they stack N=6 of for these to form their transformer's encoder \"block\". But we'll get to that in the next section.  \n",
    "The `training` flag in the `call()` function is there so that we don't perform dropout regularization during testing and inference.  \n",
    "The `padding_mask` argument, as explained in previous chapters, is to suppress zero padding tokens in input sequences from being processed along with valid input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392efb5a-69f0-484a-b838-909862fb9d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:06.494870Z",
     "iopub.status.busy": "2023-08-31T18:51:06.494785Z",
     "iopub.status.idle": "2023-08-31T18:51:06.498010Z",
     "shell.execute_reply": "2023-08-31T18:51:06.497613Z",
     "shell.execute_reply.started": "2023-08-31T18:51:06.494861Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.add_norm1 = AddAndNorm()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.add_norm2 = AddAndNorm()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(\n",
    "            feedforward_output, training=training\n",
    "        )\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c730a-0846-4280-a51a-bcf62cef9f31",
   "metadata": {},
   "source": [
    "### The Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed041dd-d6b3-4278-8385-07a3026d5571",
   "metadata": {},
   "source": [
    "We are now finally ready to stack these encoder layers to form our transformer encoder. It receives our input sequences, which have gone through tokenization, wod embedding and positional encoding. (We are re-using our `CustomEmbeddingWithFixedPosnWts` class from chapter 14 for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43dad99-da02-4a48-80ac-7b9ea923605c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:06.498797Z",
     "iopub.status.busy": "2023-08-31T18:51:06.498559Z",
     "iopub.status.idle": "2023-08-31T18:51:06.502522Z",
     "shell.execute_reply": "2023-08-31T18:51:06.502032Z",
     "shell.execute_reply.started": "2023-08-31T18:51:06.498784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        sequence_length,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_enc_layers,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wrd_emb_posn_enc = CustomEmbeddingWithFixedPosnWts(\n",
    "            sequence_length, vocab_size, d_model\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(n_heads, d_model, d_ff, dropout_rate)\n",
    "            for _ in range(n_enc_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the word embeddings & positional encodings\n",
    "        emb_enc_output = self.wrd_emb_posn_enc(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(emb_enc_output, training=training)\n",
    "        # Feed the result into the stack of encoder layers\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x, padding_mask, training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe89cd8-0ba9-4242-aa5f-1d7ff30ec37e",
   "metadata": {},
   "source": [
    "## 17.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63122b-4609-4b16-9a1b-248d04d59ad9",
   "metadata": {},
   "source": [
    "As usual, we will use the parameter values specified in AIAYN and dummy data for our input sequences (until chapter 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c13b640-3157-4de5-b5b3-fcd51fb88d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T18:51:06.503325Z",
     "iopub.status.busy": "2023-08-31T18:51:06.503130Z",
     "iopub.status.idle": "2023-08-31T18:51:06.757841Z",
     "shell.execute_reply": "2023-08-31T18:51:06.757400Z",
     "shell.execute_reply.started": "2023-08-31T18:51:06.503306Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.0252861  -0.65122014  0.75789094 ...  0.51969934 -0.5768176\n",
      "   -0.08211935]\n",
      "  [-0.57579464 -1.5733691   1.0424877  ...  0.5419647  -1.0793526\n",
      "   -1.0784875 ]\n",
      "  [-1.1129119  -1.3150351   0.93497884 ...  0.23220937 -0.32270747\n",
      "   -0.95607436]\n",
      "  [-1.5666842  -1.6021085   0.711038   ...  0.3693603  -0.19472978\n",
      "   -1.6604923 ]\n",
      "  [-0.7694094  -1.5885527   0.26493427 ...  0.3434668  -1.2378778\n",
      "   -0.38437283]]\n",
      "\n",
      " [[-0.5357287  -0.96353996  0.6988315  ... -0.31876174 -1.8667405\n",
      "   -0.3364814 ]\n",
      "  [-0.36259177 -1.0271151   0.62566733 ...  0.01233814 -1.5036724\n",
      "   -0.9422156 ]\n",
      "  [-0.5277563  -1.2224066   0.94278455 ...  0.03369743 -1.6179823\n",
      "   -0.81791395]\n",
      "  [-0.74607015 -1.5395398   0.47495332 ... -0.56117916 -2.2700295\n",
      "   -1.0501226 ]\n",
      "  [-0.8056626  -1.1419222   0.56866246 ...  0.36984262 -1.3848636\n",
      "   -0.5929973 ]]\n",
      "\n",
      " [[-0.2064347  -0.8700371   0.4188253  ...  0.26452792 -0.58523965\n",
      "   -0.5542833 ]\n",
      "  [ 0.14350943 -1.4189101   1.0279005  ...  0.13732392 -0.6589652\n",
      "   -0.93755406]\n",
      "  [-0.39215356 -1.335786    0.06030706 ...  0.5808645  -0.26122707\n",
      "   -1.2203308 ]\n",
      "  [ 0.80195165 -1.6873021   0.0880507  ...  0.16839528 -0.5895633\n",
      "   -0.06384099]\n",
      "  [ 0.13789281 -0.7588365   0.34163514 ... -0.15550032 -1.1337652\n",
      "   -0.4818216 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.1961186  -0.3087919   1.106399   ...  0.14349763 -0.76263565\n",
      "   -0.13005981]\n",
      "  [ 0.56232566 -0.80594647  1.3674937  ... -0.11535906 -0.5329196\n",
      "    0.13846645]\n",
      "  [ 0.4082407  -0.7744218   1.2542539  ... -0.25891978 -0.47391832\n",
      "    0.07026444]\n",
      "  [-0.09374861 -0.29758286  0.69472474 ... -0.22994791 -1.1734974\n",
      "   -0.17733243]\n",
      "  [-0.7318575  -1.1311957   1.65611    ...  0.16483566 -0.53281415\n",
      "    0.05449878]]\n",
      "\n",
      " [[-0.43262997 -0.8146339   0.7955256  ...  0.34026536 -0.92364967\n",
      "   -0.5843605 ]\n",
      "  [-0.953425   -1.0670112   1.2352217  ...  0.5670871  -1.6598171\n",
      "   -0.1494934 ]\n",
      "  [ 0.09268836 -0.7191605   1.1889496  ...  0.51303625 -1.2418066\n",
      "    0.29070172]\n",
      "  [ 0.07023889 -1.0038618   1.5455958  ...  0.5903051  -1.2087624\n",
      "    0.38783577]\n",
      "  [-0.96665204 -1.0142565   0.6484978  ...  0.6372842  -0.64729714\n",
      "   -1.0910319 ]]\n",
      "\n",
      " [[-0.42313507 -0.97017384  1.1074041  ...  0.17968826 -1.5101417\n",
      "   -0.27506223]\n",
      "  [-0.25723103 -1.3180017   1.362013   ...  0.29296258 -1.4053292\n",
      "    0.10658264]\n",
      "  [ 0.37261415 -0.9065034   0.5430905  ... -0.13883746 -1.0691029\n",
      "   -0.05797919]\n",
      "  [-0.28233194 -1.334871    0.36533743 ...  0.54142225 -0.866088\n",
      "   -0.27955303]\n",
      "  [-0.13976593 -0.7723438   0.12973654 ...  0.6986886  -1.2603228\n",
      "   -0.44063476]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h = 8 # Number of self-attention heads\n",
    "d_ff = 2048 # Dimensionality of the inner fully-connected layer\n",
    "d_model = 512 # Dimensionality of the model\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "batch_size = 64 # Batch size from the training process\n",
    "dropout_rate = 0.1 # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8e7edd-519e-4344-8d27-5741267db1ee",
   "metadata": {},
   "source": [
    "# Implementing the Transformer Decoder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f29d53-a128-4239-b623-9b0b58793447",
   "metadata": {},
   "source": [
    "Here again what the decoder does is that it takes the input (target) sequence's \"raw\" embeddings and, through multiple self-attention + cross-attention layers, gradually _transforms_ them to projections that are more representative and meaningful based on overall context. This is, of course, done in parallel over all tokens all at once. (At inference we generate one token at a time, but we'll get to inference in future chapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7860ee1a-2563-4f8c-9305-ee44ff87de1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-02T19:00:19.498116Z",
     "iopub.status.busy": "2023-09-02T19:00:19.487407Z",
     "iopub.status.idle": "2023-09-02T19:00:21.907896Z",
     "shell.execute_reply": "2023-09-02T19:00:21.907545Z",
     "shell.execute_reply.started": "2023-09-02T19:00:19.498070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "from tensorflow import shape\n",
    "from tensorflow.keras.layers import Dropout, Layer\n",
    "\n",
    "from xformer.common import AddAndNorm, FeedForward\n",
    "from xformer.multihead_attention import MultiHeadAttention\n",
    "from xformer.positional_encoding import CustomEmbeddingWithFixedPosnWts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b98914-a087-4828-bb22-a24a3426cce0",
   "metadata": {},
   "source": [
    "## 18.1 Recap of the Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fcfeaa-59fa-4524-b15b-18163fe11350",
   "metadata": {},
   "source": [
    "In NLP, typically a sequence-to-sequence model such as translation would have be an encoder+decoder transformer. Once again, the architecture of the decoder has _a lot_ in common with the encoder. Let's focus on its few differences and important details.  \n",
    "\n",
    "It has _three_ sub-layers instead of two:\n",
    "1. One multi-head self-attention with queries, keys and values coming from the embedded and positionally-encoded input sequences. This is architecturally identical to the multi-head self-attention layer in the encoder. (Just remember that the inputs come from _target_ sentences).\n",
    "2. It has an additional multi-head attention sub-layer which is _not_ self-attending. It gets its keys and values coming from the output of the transformer's encoder and it gets is queries from its own multi-head self-attention sub-layer (#1 above). We can call this one multi-head _cross_-attention if you want.\n",
    "3. As with the encoder, it has a fully-connected feed-forward sub-layer after that.  \n",
    "\n",
    "As with the encoder, each sub-layer above is followed by an \"Add-and-Norm\" layer normalization sub-layer. And just as before, regularization is performed by applying a dropout layer to the outputs of each of the above 3 sub-layers right before the normalization step, as well as to the positionally-encoded embeddings right before they are fed into the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191929c1-9ace-4c6c-a9e2-34e311dfae15",
   "metadata": {},
   "source": [
    "## 18.2 Implementing the Transformer Decoder from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787351f-a29d-4dd4-8f69-5b020cad8227",
   "metadata": {},
   "source": [
    "Let's just jump right into implementing it, starting by defining the `DecoderLayer` and `Decoder` classes, simililarly to how we did things for the encoder and reusing a lot of the code from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1c187d-7cd3-44b3-8809-2f937c72b759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-02T19:00:21.908981Z",
     "iopub.status.busy": "2023-09-02T19:00:21.908784Z",
     "iopub.status.idle": "2023-09-02T19:00:21.912850Z",
     "shell.execute_reply": "2023-09-02T19:00:21.912589Z",
     "shell.execute_reply.started": "2023-09-02T19:00:21.908970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.add_norm1 = AddAndNorm()\n",
    "        self.multihead_attention2 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.add_norm2 = AddAndNorm()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "        self.add_norm3 = AddAndNorm()\n",
    "\n",
    "    def call(self, x, mask, encoder_output, encoder_mask, training):\n",
    "        # Multi-head self-attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head (cross-)attention layer\n",
    "        multihead_output2 = self.multihead_attention2(\n",
    "            addnorm_output1, encoder_output, encoder_output, encoder_mask\n",
    "        )\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm2(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(\n",
    "            feedforward_output, training=training\n",
    "        )\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef02f5e-1969-4b86-be52-3b0a5fbdb1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-02T19:00:21.913310Z",
     "iopub.status.busy": "2023-09-02T19:00:21.913221Z",
     "iopub.status.idle": "2023-09-02T19:00:21.916356Z",
     "shell.execute_reply": "2023-09-02T19:00:21.916075Z",
     "shell.execute_reply.started": "2023-09-02T19:00:21.913301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        sequence_length,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_dec_layers,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wrd_emb_posn_enc = CustomEmbeddingWithFixedPosnWts(\n",
    "            sequence_length, vocab_size, d_model\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.decoder_layer = [\n",
    "            DecoderLayer(n_heads, d_model, d_ff, dropout_rate)\n",
    "            for _ in range(n_dec_layers)\n",
    "        ]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        target_sequence,\n",
    "        mask,\n",
    "        encoder_output,\n",
    "        encoder_mask,\n",
    "        training,\n",
    "    ):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.wrd_emb_posn_enc(target_sequence)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, mask, encoder_output, encoder_mask, training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf944d6-3bd4-4dad-a8dc-8b552895f074",
   "metadata": {},
   "source": [
    "## 18.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cfef2-6a67-47ed-8296-0f7702732bae",
   "metadata": {},
   "source": [
    "As before, let's test it out with parameter values from AIAYN. We'll use dummy data for the target sequences _and_ for our encoder output. Also we won't be using masks yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d62ab5e-2b1f-4440-8ba1-03e57140d2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-02T19:00:21.916863Z",
     "iopub.status.busy": "2023-09-02T19:00:21.916778Z",
     "iopub.status.idle": "2023-09-02T19:00:22.209542Z",
     "shell.execute_reply": "2023-09-02T19:00:22.209220Z",
     "shell.execute_reply.started": "2023-09-02T19:00:21.916855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.14689298  1.4889189   1.4373505  ...  0.2518994  -1.1189088\n",
      "   -1.5063888 ]\n",
      "  [ 0.36743933  0.85430753  0.62036705 ...  0.05930083 -0.34579793\n",
      "   -1.7476729 ]\n",
      "  [-0.42957708  1.2475446   1.4967277  ...  0.10902691 -1.3981266\n",
      "   -0.5514256 ]\n",
      "  [-0.73810756  1.0564915   0.44105342 ...  0.24699281 -1.1235929\n",
      "   -1.281653  ]\n",
      "  [-0.7004271   0.7580604   0.511743   ...  0.04367047 -0.69809765\n",
      "   -1.1786984 ]]\n",
      "\n",
      " [[-0.33187953  1.6393027   0.7590318  ...  0.10175119 -0.5207812\n",
      "   -0.96342885]\n",
      "  [ 0.15893792  1.5600148   1.3638816  ... -0.28914726 -0.15736698\n",
      "   -1.3809341 ]\n",
      "  [-0.1530173   0.9120058   0.9395371  ... -0.4249657  -0.03356398\n",
      "   -1.9221315 ]\n",
      "  [ 0.10429339  1.4003291   1.7235233  ... -0.1958521   0.2185743\n",
      "   -1.5828359 ]\n",
      "  [ 0.57673734  2.2125227  -0.12343413 ...  0.39349705 -0.30001473\n",
      "   -1.1428877 ]]\n",
      "\n",
      " [[ 0.39791936  0.07334416  0.9556452  ... -0.02623349 -0.5667957\n",
      "   -2.1628373 ]\n",
      "  [ 0.71776915  0.7817034   0.40280566 ... -0.6789003   0.02750724\n",
      "   -1.6776593 ]\n",
      "  [ 0.3480348   1.3710699   0.98564214 ...  0.10129404 -0.13126214\n",
      "   -1.3046952 ]\n",
      "  [ 0.4568741   0.27410868  0.7612371  ...  0.07246198 -0.2076843\n",
      "   -2.1054363 ]\n",
      "  [ 0.22212222  0.7733466   0.93264604 ...  0.21887662 -0.41099638\n",
      "   -1.6537344 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.56037277  0.57721615  0.569349   ...  0.15559283 -0.94651085\n",
      "   -1.5301486 ]\n",
      "  [-0.1697019   1.8566345   0.6169602  ...  0.19617064 -0.5484907\n",
      "   -1.2028124 ]\n",
      "  [ 0.5519812   1.4877658   0.6975625  ...  0.20836146 -0.43554416\n",
      "   -0.8877314 ]\n",
      "  [-0.59288394  1.0324875   1.0300183  ...  0.39741975 -1.0843352\n",
      "   -0.747357  ]\n",
      "  [ 0.16379626  1.2431195   1.1311811  ... -0.10542013 -0.47437498\n",
      "   -1.1844093 ]]\n",
      "\n",
      " [[-0.0561247   1.0001469   0.60255265 ... -0.5724035  -0.13945866\n",
      "   -1.3260158 ]\n",
      "  [ 0.04161471  1.8690168   0.9000445  ... -0.44072378  0.14055616\n",
      "   -1.5830127 ]\n",
      "  [-0.02023535  0.8985409   0.24879973 ... -0.52895635  0.31131223\n",
      "   -1.6149416 ]\n",
      "  [ 0.5979143   0.34439737  0.99920464 ...  0.2421399  -0.4111496\n",
      "   -1.3674815 ]\n",
      "  [-0.31831405  0.61401784  0.8871746  ... -0.5998429  -0.7801063\n",
      "   -1.7452714 ]]\n",
      "\n",
      " [[ 0.18455102 -0.2024034   0.2196519  ...  0.19059694  0.67704207\n",
      "   -2.1604578 ]\n",
      "  [ 0.17844655  0.8042346   0.41092888 ... -0.2023841   0.520724\n",
      "   -2.0818517 ]\n",
      "  [ 0.60463065  0.3897708   0.1980761  ... -0.09363333 -0.06085229\n",
      "   -1.3045322 ]\n",
      "  [ 0.3365502   1.4439901  -0.11205131 ...  0.05348969  0.6270383\n",
      "   -0.5675842 ]\n",
      "  [-0.53263646 -0.2662465   0.71311617 ... -0.09988201 -0.10045414\n",
      "   -1.7980608 ]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h = 8  # Number of self-attention heads\n",
    "d_ff = 2048  # Dimensionality of the inner fully-connected layer\n",
    "d_model = 512  # Dimensionality of the model\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in dropout layers\n",
    "\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(\n",
    "    dec_vocab_size, input_seq_length, h, d_model, d_ff, n, dropout_rate\n",
    ")\n",
    "\n",
    "print(decoder(input_seq, None, enc_output, None, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

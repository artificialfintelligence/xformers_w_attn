{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8e7edd-519e-4344-8d27-5741267db1ee",
   "metadata": {},
   "source": [
    "# Implementing the Transformer Decoder in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7860ee1a-2563-4f8c-9305-ee44ff87de1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T19:20:17.553380Z",
     "iopub.status.busy": "2023-08-31T19:20:17.552500Z",
     "iopub.status.idle": "2023-08-31T19:20:17.570680Z",
     "shell.execute_reply": "2023-08-31T19:20:17.565612Z",
     "shell.execute_reply.started": "2023-08-31T19:20:17.553337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "from tensorflow.keras.layers import Dropout, Layer\n",
    "\n",
    "from xformer.common import AddAndNorm, FeedForward\n",
    "from xformer.multihead_attention import MultiHeadAttention\n",
    "from xformer.positional_encoding import CustomEmbeddingWithFixedPosnWts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b98914-a087-4828-bb22-a24a3426cce0",
   "metadata": {},
   "source": [
    "## 18.1 Recap of the Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fcfeaa-59fa-4524-b15b-18163fe11350",
   "metadata": {},
   "source": [
    "In NLP, typically a sequence-to-sequence model such as translation would have be an encoder+decoder transformer. Once again, the architecture of the decoder has _a lot_ in common with the encoder. Let's focus on its few differences and important details.  \n",
    "\n",
    "It has _three_ sub-layers instead of two:\n",
    "1. One multi-head self-attention with queries, keys and values coming from the embedded and positionally-encoded input sequences. This is architecturally identical to the multi-head self-attention layer in the encoder. (Just remember that the inputs come from _target_ sentences).\n",
    "2. It has an additional multi-head attention sub-layer which is _not_ self-attending. It gets its keys and values coming from the output of the transformer's encoder and it gets is queries from its own multi-head self-attention sub-layer (#1 above). We can call this one multi-head _cross_-attention if you want.\n",
    "3. As with the encoder, it has a fully-connected feed-forward sub-layer after that.  \n",
    "\n",
    "As with the encoder, each sub-layer above is followed by an \"Add-and-Norm\" layer normalization sub-layer. And just as before, regularization is performed by applying a dropout layer to the outputs of each of the above 3 sub-layers right before the normalization step, as well as to the positionally-encoded embeddings right before they are fed into the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191929c1-9ace-4c6c-a9e2-34e311dfae15",
   "metadata": {},
   "source": [
    "## 18.2 Implementing the Transformer Decoder from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787351f-a29d-4dd4-8f69-5b020cad8227",
   "metadata": {},
   "source": [
    "Let's just jump right into implementing it, starting by defining the `DecoderLayer` and `Decoder` classes, simililarly to how we did things for the encoder and reusing a lot of the code from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d1c187d-7cd3-44b3-8809-2f937c72b759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T19:28:58.309724Z",
     "iopub.status.busy": "2023-08-31T19:28:58.308759Z",
     "iopub.status.idle": "2023-08-31T19:28:58.325042Z",
     "shell.execute_reply": "2023-08-31T19:28:58.324363Z",
     "shell.execute_reply.started": "2023-08-31T19:28:58.309685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.add_norm1 = AddAndNorm()\n",
    "        self.multihead_attention2 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.add_norm2 = AddAndNorm()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "        self.add_norm3 = AddAndNorm()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head self-attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by another multi-head (cross-)attention layer\n",
    "        multihead_output2 = self.multihead_attention2(\n",
    "            addnorm_output1, encoder_output, encoder_output, padding_mask\n",
    "        )\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(\n",
    "            feedforward_output, training=training\n",
    "        )\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef02f5e-1969-4b86-be52-3b0a5fbdb1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T19:28:56.966871Z",
     "iopub.status.busy": "2023-08-31T19:28:56.965979Z",
     "iopub.status.idle": "2023-08-31T19:28:56.984026Z",
     "shell.execute_reply": "2023-08-31T19:28:56.982135Z",
     "shell.execute_reply.started": "2023-08-31T19:28:56.966827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        sequence_length,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = CustomEmbeddingWithFixedPosnWts(\n",
    "            sequence_length, vocab_size, d_model\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.decoder_layer = [\n",
    "            DecoderLayer(n_heads, d_model, d_ff, dropout_rate) for _ in range(n)\n",
    "        ]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        output_target,\n",
    "        encoder_output,\n",
    "        lookahead_mask,\n",
    "        padding_mask,\n",
    "        training,\n",
    "    ):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf944d6-3bd4-4dad-a8dc-8b552895f074",
   "metadata": {},
   "source": [
    "## 18.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cfef2-6a67-47ed-8296-0f7702732bae",
   "metadata": {},
   "source": [
    "As beforelet's test it out with parameter values from AIAYN. We'll use dummy data for the target sequences _and_ for our encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d62ab5e-2b1f-4440-8ba1-03e57140d2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T19:31:16.901234Z",
     "iopub.status.busy": "2023-08-31T19:31:16.900185Z",
     "iopub.status.idle": "2023-08-31T19:31:17.182287Z",
     "shell.execute_reply": "2023-08-31T19:31:17.181812Z",
     "shell.execute_reply.started": "2023-08-31T19:31:16.901146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h = 8  # Number of self-attention heads\n",
    "d_ff = 2048  # Dimensionality of the inner fully-connected layer\n",
    "d_model = 512  # Dimensionality of the model\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in dropout layers\n",
    "\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(\n",
    "    dec_vocab_size, input_seq_length, h, d_model, d_ff, n, dropout_rate\n",
    ")\n",
    "\n",
    "print(decoder(input_seq, enc_output, None, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb9ba2b-101c-458e-ad7a-5ee724de5345",
   "metadata": {},
   "source": [
    "# 22 Inference with the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb3968b-ddcf-400e-bbb5-408ebf72fb90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T23:34:49.806137Z",
     "iopub.status.busy": "2023-09-23T23:34:49.804483Z",
     "iopub.status.idle": "2023-09-23T23:34:52.883294Z",
     "shell.execute_reply": "2023-09-23T23:34:52.882978Z",
     "shell.execute_reply.started": "2023-09-23T23:34:49.805821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "from tensorflow import (\n",
    "    Module,\n",
    "    TensorArray,\n",
    "    argmax,\n",
    "    convert_to_tensor,\n",
    "    int64,\n",
    "    newaxis,\n",
    "    transpose,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from xformer.model import Xformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975f328-7c99-421c-ba2e-4e5accfccead",
   "metadata": {},
   "source": [
    "## 22.1 Performing Inference the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca40e20-4e35-44fc-b882-5860f7c0c349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T23:34:52.884345Z",
     "iopub.status.busy": "2023-09-23T23:34:52.884143Z",
     "iopub.status.idle": "2023-09-23T23:34:52.987139Z",
     "shell.execute_reply": "2023-09-23T23:34:52.986763Z",
     "shell.execute_reply.started": "2023-09-23T23:34:52.884335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the dataset parameters\n",
    "enc_seq_length = 7  # Encoder sequence length\n",
    "dec_seq_length = 12  # Decoder sequence length\n",
    "enc_vocab_size = 2405  # Encoder vocabulary size\n",
    "dec_vocab_size = 3864  # Decoder vocabulary size\n",
    "\n",
    "# Create model\n",
    "trained_model = Xformer(\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    h,\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    n,\n",
    "    0,\n",
    ")  # Note that dropout_rate is zero for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749b754-f580-4b8a-b8e9-002ded111f8e",
   "metadata": {},
   "source": [
    "We need to use the same tokenizer that we used for the encoder for training to tokenize our test sequences. We use the decoder tokenizer to tokenize the special START and END of sequence tokens for the output.  \n",
    "When preparing the output array, we don't know what its size is going to be, so we initialize it with size zero, but set the `dynamic_size` parameter to `True` to allow it it to grow dynamically. Then we add the START token to it.  \n",
    "Notice how at inference time, the transformer model works iteratively. It's called 'autoregressive generation'. There is no 'teacher forcing' as with during training. So the decoder needs to generate output one token at a time (using some strategy; we've chosen 'greedy' decoding. But do read up on 'beam search', which is another important alternative). We iterate until the maximum decoder output length is reached, or the special EOS token is predicted and generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c351bf7e-3b2d-4daf-98e0-1bee8c13552d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T23:34:52.987713Z",
     "iopub.status.busy": "2023-09-23T23:34:52.987621Z",
     "iopub.status.idle": "2023-09-23T23:34:52.992909Z",
     "shell.execute_reply": "2023-09-23T23:34:52.992363Z",
     "shell.execute_reply.started": "2023-09-23T23:34:52.987704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Translate(Module):\n",
    "    def __init__(self, inferencing_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transformer = trained_model\n",
    "\n",
    "    def load_tokenizer(self, name):\n",
    "        with open(name, \"rb\") as handle:\n",
    "            return load(handle)\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        sentence[0] = \"<SEQSTART> \" + sentence[0] + \" <EOS>\"\n",
    "\n",
    "        enc_tokenizer = self.load_tokenizer(\"data/enc_tokenizer.pkl\")\n",
    "        dec_tokenizer = self.load_tokenizer(\"data/dec_tokenizer.pkl\")\n",
    "\n",
    "        # Prepare the input sentence by tokenizing, padding and converting to tensor\n",
    "        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n",
    "        encoder_input = pad_sequences(\n",
    "            encoder_input, maxlen=enc_seq_length, padding=\"post\"\n",
    "        )\n",
    "        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n",
    "        \n",
    "        # Prepare the output <SEQSTART> token by tokenizing, and converting to tensor\n",
    "        output_start = dec_tokenizer.texts_to_sequences([\"<SEQSTART>\"])\n",
    "        output_start = convert_to_tensor(output_start[0], dtype=int64)\n",
    "        \n",
    "        # Prepare the output <EOS> token by tokenizing, and converting to tensor\n",
    "        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\n",
    "        output_end = convert_to_tensor(output_end[0], dtype=int64)\n",
    "        \n",
    "        # Prepare the output array of dynamic size\n",
    "        decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\n",
    "        decoder_output = decoder_output.write(0, output_start)\n",
    "        \n",
    "        for i in range(dec_seq_length):\n",
    "            # Predict an output token\n",
    "            prediction = self.transformer(encoder_input,transpose(decoder_output.stack()), training=False)\n",
    "            prediction = prediction[:, -1, :]\n",
    "            \n",
    "            # Select the prediction with the highest score\n",
    "            predicted_id = argmax(prediction, axis=-1)\n",
    "            predicted_id = predicted_id[0][newaxis]\n",
    "            \n",
    "            # Write the selected prediction to the output array at the next\n",
    "            # available index\n",
    "            decoder_output = decoder_output.write(i + 1, predicted_id)\n",
    "            \n",
    "            # Break if an <EOS> token is predicted\n",
    "            if predicted_id == output_end:\n",
    "                break\n",
    "            \n",
    "        output = transpose(decoder_output.stack())[0]\n",
    "        output = output.numpy()\n",
    "\n",
    "        output_str = []\n",
    "\n",
    "        # Decode the predicted tokens into an output string\n",
    "        for i in range(output.shape[0]):\n",
    "            key = output[i]\n",
    "            output_str.append(dec_tokenizer.index_word[key])\n",
    "\n",
    "        return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747fc54-4f89-4f9f-b198-9bce0dff639a",
   "metadata": {},
   "source": [
    "## 22.2 Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59f784-94ba-4ef0-8265-5de5c38e03e2",
   "metadata": {},
   "source": [
    "My validation loss kind of plateaued after the 8th epoch, so we will use the weights from that epoch for inference. (See learning curves in the previous chapter).  \n",
    "**Note:** In the book, their learning curves look better and validation loss only starts to plateau after the 16th epoch. But mine actually gets _worse_ starting around the 10th-12th epoch. I have some nagging suspicions that the book's implementation has some bugs and indeed it is even more likley that my own code has introduced bugs as I tried to correct some of the mistakes and shortcomings of the code from the book. But we will let it all slide. This was a learning project anyway, and I have learned a great deal, in a very in-depth and hands-on manner, about the inner workings of attention mechanisms and transformer models. \\*pat myself on the back\\*  \n",
    "Remember: Transformers are notoriously data-hungry. The more training data we feed it, the better it's going to perform (to a point, supposedly).  \n",
    "And now for the last hurrah..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09953d4c-d79e-4301-bf24-86f22714657c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T23:36:46.822658Z",
     "iopub.status.busy": "2023-09-23T23:36:46.821723Z",
     "iopub.status.idle": "2023-09-23T23:36:47.584977Z",
     "shell.execute_reply": "2023-09-23T23:36:47.584656Z",
     "shell.execute_reply.started": "2023-09-23T23:36:46.822620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seqstart', 'es', 'ist', 'nicht', 'eos']\n"
     ]
    }
   ],
   "source": [
    "# Sentence to translate\n",
    "sentence = ['im thirsty']\n",
    "# Ideally we should get ‚Äú<SEQSTART> ich bin durstig <EOS>\"\n",
    "\n",
    "# Load the trained model's weights at the specified epoch\n",
    "trained_model.load_weights('weights/wghts8.ckpt')\n",
    "\n",
    "# Create a new instance of the 'Translate' class\n",
    "translator = Translate(trained_model)\n",
    "\n",
    "# Translate the input sentence\n",
    "print(translator(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40414b-217a-418d-9940-5d379daeca9e",
   "metadata": {},
   "source": [
    "...  \n",
    "It's... something!! üòÖü§∑‚Äç‚ôÇÔ∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

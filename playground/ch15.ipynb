{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2269a99-856f-4ec9-a9de-8dd685f43186",
   "metadata": {},
   "source": [
    "# 15 Implementing Scaled Dot-Product Attention in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede5a9a9-6a02-4db9-8baf-77de23d3a213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T20:25:07.487096Z",
     "iopub.status.busy": "2023-08-27T20:25:07.486474Z",
     "iopub.status.idle": "2023-08-27T20:25:07.500744Z",
     "shell.execute_reply": "2023-08-27T20:25:07.500174Z",
     "shell.execute_reply.started": "2023-08-27T20:25:07.487038Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.backend import softmax\n",
    "from tensorflow import cast, float32, math, matmul\n",
    "from tensorflow.keras.layers import Layer\n",
    "from numpy import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f748b45-c55f-433d-a91d-b236cd03b1f6",
   "metadata": {},
   "source": [
    "## 15.1 Recap of the Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca6391-0ade-457c-8041-a255c2b81e9b",
   "metadata": {},
   "source": [
    "The encoder and the decoder share much of their architecture. At the heart of their numerous, stacked, multi-head attention blocks is the *scaled dot-product attention* mechanism.\n",
    "In the multi-head attention block of the encoder, the query, key and value vectors (which form the Query, Key and Value matrices once concatenated) are simply the encoded and embedded (see ch. 14) input sequence. Similarly, on the decoder side, the first attention block gets the encoded/embedded _target_ sequence in the form of query, key and value vectors. However, the _second_ attention block receives the final output of the encoder block for its keys and values but uses the [normalized] output of its own first attention block as its queries. (The latter can be thought of as the decoder output from the \"previous time step\", but do keep in mind that there is no recurrence here and everything is fed to the model all at once). \n",
    "We will denote the dimensionality of queries and keys with $d_k$ and that of values with $d_v$.\n",
    "First we calculate the matrix multiplication of $Q$ and $K^T$ (which is equivalent to calculating the dot products of query and key _vectors_). Then we scale the result by the square root of $d_k$ to get the _attention scores_. We feed the result to the $softmax$ function to get _attention weights_. And finally, we scale the the value vectors by matrix-multiplying the result with $V$.\n",
    "$$attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e465a-b2d6-4559-b8a2-86345d5fe7a0",
   "metadata": {},
   "source": [
    "A \"mask\" can optionally be applied to the attention scores before they are fed to the $softmax$ function. Here are two conceivable applications for this:  \n",
    "- A \"look-ahead mask\" (as in the first attention block of the decoder) can prevent the model from, you guessed it, \"looking ahead\" and attending to succeeding tokens in the target sequence. (\"Succeeding\" in the sense that it has not yet reached them and output(ted) a prediction for those positions in the target sequence).\n",
    "- A \"padding mask\" can prevent the padding (often zero) tokens from being processed along with meaningful tokens both in the encoder and decoder stages.\n",
    "Masking works by replacing the attention scores to be masked with $-\\infty$ so that $softmax$ will result in zeros for those positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c8c75-14bf-48df-b152-6e860cb3681d",
   "metadata": {},
   "source": [
    "## 15.2 Implementing the Scaled Dot-Product Attention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e94e6dc1-7ca5-48ee-977c-9488a743730d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T20:25:10.321657Z",
     "iopub.status.busy": "2023-08-27T20:25:10.320249Z",
     "iopub.status.idle": "2023-08-27T20:25:10.334132Z",
     "shell.execute_reply": "2023-08-27T20:25:10.333229Z",
     "shell.execute_reply.started": "2023-08-27T20:25:10.321554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing the Scaled Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Score the queries against the keys after transposing the latter, and then scale\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += float(\"-inf\") * mask\n",
    "        # Compute the weights using a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        # Compute attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02385bb8-7f45-4e17-b57e-35def29c8d52",
   "metadata": {},
   "source": [
    "## 15.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "525e2244-53a3-40f4-b40f-8e48efaa1d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T20:25:11.931561Z",
     "iopub.status.busy": "2023-08-27T20:25:11.931187Z",
     "iopub.status.idle": "2023-08-27T20:25:11.968183Z",
     "shell.execute_reply": "2023-08-27T20:25:11.967519Z",
     "shell.execute_reply.started": "2023-08-27T20:25:11.931535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.42829984 0.5291363  0.48467714 ... 0.60236514 0.6314437  0.3679649 ]\n",
      "  [0.42059594 0.51898783 0.46809807 ... 0.5975176  0.63140476 0.3960448 ]\n",
      "  [0.4529176  0.53372943 0.482216   ... 0.5861657  0.6170542  0.35611773]\n",
      "  [0.4353886  0.52972203 0.4782614  ... 0.5917442  0.62593013 0.3666562 ]\n",
      "  [0.42998835 0.51891106 0.481131   ... 0.610327   0.63044834 0.39192218]]\n",
      "\n",
      " [[0.61051536 0.5024951  0.401304   ... 0.7148773  0.36341453 0.5512419 ]\n",
      "  [0.5842008  0.5239525  0.4311911  ... 0.72335523 0.36001056 0.5697574 ]\n",
      "  [0.56449413 0.559814   0.4412013  ... 0.6975891  0.34060013 0.57147545]\n",
      "  [0.5878388  0.52120656 0.42275843 ... 0.7043989  0.34812245 0.556117  ]\n",
      "  [0.5880349  0.52016133 0.43390357 ... 0.7050327  0.35547623 0.5617097 ]]\n",
      "\n",
      " [[0.42078587 0.49814874 0.49267095 ... 0.49847665 0.4983842  0.2441965 ]\n",
      "  [0.4155558  0.4971297  0.49442884 ... 0.5093836  0.5085627  0.24680805]\n",
      "  [0.42648807 0.4867728  0.47802505 ... 0.49915406 0.4939838  0.24511527]\n",
      "  [0.42818245 0.48918223 0.5092939  ... 0.49413374 0.50010735 0.24244288]\n",
      "  [0.41987744 0.47195825 0.49843693 ... 0.47901452 0.48088    0.23752582]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.34208363 0.33005255 0.6106712  ... 0.37145126 0.39500523 0.4489295 ]\n",
      "  [0.32618567 0.33101344 0.6183841  ... 0.36305496 0.39253712 0.44402236]\n",
      "  [0.32666227 0.34087297 0.6013796  ... 0.3783583  0.40691683 0.45996755]\n",
      "  [0.31646597 0.33332765 0.63053775 ... 0.3667221  0.39233196 0.44519982]\n",
      "  [0.32949486 0.3404245  0.60722286 ... 0.36759531 0.3955865  0.44391456]]\n",
      "\n",
      " [[0.6142529  0.4523761  0.40557322 ... 0.4931701  0.5037514  0.5581999 ]\n",
      "  [0.6070069  0.46827996 0.41286847 ... 0.5041559  0.48487282 0.54769754]\n",
      "  [0.5920976  0.4585476  0.41433176 ... 0.50770736 0.5127847  0.54510796]\n",
      "  [0.6094537  0.45495343 0.42266536 ... 0.50298333 0.49534026 0.5457492 ]\n",
      "  [0.58945996 0.46224147 0.4051993  ... 0.50452286 0.50907815 0.5500692 ]]\n",
      "\n",
      " [[0.48170575 0.41727182 0.3243676  ... 0.51853126 0.48532185 0.4992545 ]\n",
      "  [0.4819431  0.41776437 0.32909    ... 0.51749444 0.48206937 0.4877416 ]\n",
      "  [0.48521042 0.41496757 0.31763408 ... 0.5068725  0.48783246 0.49117002]\n",
      "  [0.47845012 0.41720834 0.321815   ... 0.512636   0.48480433 0.49299487]\n",
      "  [0.50574267 0.41419291 0.33272395 ... 0.5244207  0.4885689  0.49498346]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "batch_size = 64 # Batch size from the training process\n",
    "\n",
    "# Dummy data follows...\n",
    "# In reality, these would be obtained from the tokenized and then embedded sequences.\n",
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba167a4-546f-463b-adfa-2bc2829d4a8d",
   "metadata": {},
   "source": [
    "**Note:** The output shape is `(batch size, sequence length, dim_values)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197569a5-8d40-44d0-a059-f6b19fcdffec",
   "metadata": {},
   "source": [
    "# 20 Training the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb113c5f-7a02-429b-b32f-74f18bce72a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:26.901823Z",
     "iopub.status.busy": "2023-09-22T18:48:26.900375Z",
     "iopub.status.idle": "2023-09-22T18:48:29.198721Z",
     "shell.execute_reply": "2023-09-22T18:48:29.198417Z",
     "shell.execute_reply.started": "2023-09-22T18:48:26.901651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from time import time\n",
    "\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from numpy.random import shuffle\n",
    "from tensorflow import (\n",
    "    GradientTape,\n",
    "    TensorSpec,\n",
    "    argmax,\n",
    "    cast,\n",
    "    convert_to_tensor,\n",
    "    data,\n",
    "    equal,\n",
    "    float32,\n",
    "    function,\n",
    "    int64,\n",
    "    math,\n",
    "    reduce_sum,\n",
    "    train,\n",
    ")\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from xformer.model import Xformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a804f0-f38f-4d6a-b30b-5d3224f6a50e",
   "metadata": {},
   "source": [
    "## 20.1 Preparing the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fbc83-b353-4467-8cae-34e28439fa72",
   "metadata": {},
   "source": [
    "The dataset is already standardized and clean (no punctuation, all lowercae, etc.) and it can be downloaded from [here](https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl).  \n",
    "The class below loads the data, selects a subset of it for demonstration purposes (because it's very large), appends special `<START>` and `<EOS>` tokens to the beginning and end of the sequences, splits them based on a pre-defined ratio (the train-test split), tokenizes the input and target sequences separately and uses these to deduce the maximum sequence length and vocabulary size for the encoder and decoder respectively.  \n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a9863d-3de8-49ec-81aa-f9d72aafcd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.199820Z",
     "iopub.status.busy": "2023-09-22T18:48:29.199599Z",
     "iopub.status.idle": "2023-09-22T18:48:29.204759Z",
     "shell.execute_reply": "2023-09-22T18:48:29.204452Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.199809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = (\n",
    "            10_000  # Number of sentences to include in the dataset\n",
    "        )\n",
    "        self.train_split = 0.9  # Proportion of the data to use for training\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, \"rb\"))\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[: self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        # Note: The book uses <START> but that is no good since it will be\n",
    "        # cleaned and lowercased to \"start\" and get mixed up with the actual\n",
    "        # English word \"start\", which does appear in the training data.\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<SEQSTART> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<SEQSTART> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "\n",
    "        # Split the dataset\n",
    "        train = dataset[: int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding=\"post\")\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding=\"post\")\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "        return (\n",
    "            trainX,\n",
    "            trainY,\n",
    "            train,\n",
    "            enc_seq_length,\n",
    "            dec_seq_length,\n",
    "            enc_vocab_size,\n",
    "            dec_vocab_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bd268-716c-487f-9aea-08cb4bb0449f",
   "metadata": {},
   "source": [
    "Let's test it and take a look at some sample sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e632f979-bac7-4f3e-b602-0f679573be4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.205257Z",
     "iopub.status.busy": "2023-09-22T18:48:29.205169Z",
     "iopub.status.idle": "2023-09-22T18:48:29.600786Z",
     "shell.execute_reply": "2023-09-22T18:48:29.600459Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.205248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    train_orig,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    ") = dataset(\"data/english-german-both.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a95723-850a-4b82-9d84-92eb3a321438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.601334Z",
     "iopub.status.busy": "2023-09-22T18:48:29.601236Z",
     "iopub.status.idle": "2023-09-22T18:48:29.606656Z",
     "shell.execute_reply": "2023-09-22T18:48:29.606073Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.601324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEQSTART> toms conscious <EOS>\n",
      "tf.Tensor([   1   44 1442    2    0    0    0], shape=(7,), dtype=int64)\n",
      "\n",
      "\n",
      "<SEQSTART> tom ist bei bewusstsein <EOS>\n",
      "tf.Tensor([  1   5   4 196 552   2   0   0   0   0   0   0], shape=(12,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    train_orig[0, 0],\n",
    "    trainX[0, :],\n",
    "    \"\\n\",\n",
    "    train_orig[0, 1],\n",
    "    trainY[0, :],\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2e978-6ffc-4449-bd03-0b7bfe757863",
   "metadata": {},
   "source": [
    "It's a dataset of very short English and German sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a18dc2-4ff4-4a19-b63c-c05eccd6e73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.609218Z",
     "iopub.status.busy": "2023-09-22T18:48:29.609078Z",
     "iopub.status.idle": "2023-09-22T18:48:29.611509Z",
     "shell.execute_reply": "2023-09-22T18:48:29.611017Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.609209Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 7\n",
      "Decoder sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder sequence length:\", enc_seq_length)\n",
    "print(\"Decoder sequence length:\", dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43088d5f-659a-4e57-a49d-ee9c04ad07eb",
   "metadata": {},
   "source": [
    "## 20.2 Applying a Padding Mask\n",
    "### (And Introducing the Loss Function and Accuracy Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c1d4b-0f0a-410f-a711-bb4f90d1905a",
   "metadata": {},
   "source": [
    "So, here's the thing: Just masking the input and target sequences was not enough. We also need to exclude the masked tokens from being used in the calculation of our loss function and our accuracy metric.  \n",
    "We will be using a sparse categorical cross-entropy loss function. Here's the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ac7011-41d4-4563-90df-37eb2dbb3b43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.612305Z",
     "iopub.status.busy": "2023-09-22T18:48:29.612115Z",
     "iopub.status.idle": "2023-09-22T18:48:29.615048Z",
     "shell.execute_reply": "2023-09-22T18:48:29.614541Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.612290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included\n",
    "    # in the computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = (\n",
    "        sparse_categorical_crossentropy(target, prediction, from_logits=True)\n",
    "        * mask\n",
    "    )\n",
    "\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8976f-b741-4797-a6c4-805aab549eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T01:18:20.328446Z",
     "iopub.status.busy": "2023-09-15T01:18:20.327739Z",
     "iopub.status.idle": "2023-09-15T01:18:21.040814Z",
     "shell.execute_reply": "2023-09-15T01:18:21.040326Z",
     "shell.execute_reply.started": "2023-09-15T01:18:20.328416Z"
    },
    "tags": []
   },
   "source": [
    "Note that the output of the decoder is a tensor of shape `(batch_size, dec_seq_length, dec_vocab_size)` and its values represent the probabilities for each vocabulary token at each position in the output sequence. In order to compare the output to the target sequence, we will pick only the highest probability token at each position (and retrieve its corresponding token/word using `argmax`) and calculate the average accuracy (which is 0 or 1 for an individual token) over all unmasked values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0bf071-1d9e-4126-a7f4-8df58c6770bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.615724Z",
     "iopub.status.busy": "2023-09-22T18:48:29.615605Z",
     "iopub.status.idle": "2023-09-22T18:48:29.619061Z",
     "shell.execute_reply": "2023-09-22T18:48:29.618407Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.615713Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(math.equal(target, 0))\n",
    "\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2)) # Should this be `argmax(prediction, axis=2) + 1` ??\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fc8b1-b52e-4291-9248-e869c36a1020",
   "metadata": {},
   "source": [
    "## 20.3 Training the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9db65-81fb-4edd-af0d-3f532d3ef85b",
   "metadata": {},
   "source": [
    "As always, we will use the parameters used in the AIAYN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1b4226-069f-44c2-a51d-794712e78664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.619689Z",
     "iopub.status.busy": "2023-09-22T18:48:29.619565Z",
     "iopub.status.idle": "2023-09-22T18:48:29.622261Z",
     "shell.execute_reply": "2023-09-22T18:48:29.621884Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.619680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ba734-af8f-4753-a0b2-c94d6810fa34",
   "metadata": {},
   "source": [
    "And we'll use a learning rate scheduler which was specified in the same paper as follows:  \n",
    "$$\\text { lrate }=d_{\\mathrm{model}}^{-0.5} \\cdot \\min \\left(step\\_num^{-0.5}, \\text { step_num } \\cdot \\text { warmup_steps }{ }^{-1.5}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ace1ae-1593-44c4-a3b3-16f7a463bb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.622926Z",
     "iopub.status.busy": "2023-09-22T18:48:29.622794Z",
     "iopub.status.idle": "2023-09-22T18:48:29.625677Z",
     "shell.execute_reply": "2023-09-22T18:48:29.625340Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.622917Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = cast(warmup_steps, float32)\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and\n",
    "        # decreasing it thereafter\n",
    "        step_num = cast(step_num, float32)\n",
    "        arg1 = step_num**-0.5\n",
    "        arg2 = step_num * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return (self.d_model**-0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac54443-1e98-4c0f-99ba-fb8bf461522b",
   "metadata": {},
   "source": [
    "Let's prepare our batches for training and instantiate our model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11806dd2-fbed-404b-b805-30dd25dadd48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.626456Z",
     "iopub.status.busy": "2023-09-22T18:48:29.626261Z",
     "iopub.status.idle": "2023-09-22T18:48:29.631416Z",
     "shell.execute_reply": "2023-09-22T18:48:29.630868Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.626443Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f726bc-5b66-42ae-9c5f-e62aef3153fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.632433Z",
     "iopub.status.busy": "2023-09-22T18:48:29.632242Z",
     "iopub.status.idle": "2023-09-22T18:48:29.635664Z",
     "shell.execute_reply": "2023-09-22T18:48:29.635321Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.632420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e15441-f59d-46b9-95d0-8be775dbe8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.636333Z",
     "iopub.status.busy": "2023-09-22T18:48:29.636202Z",
     "iopub.status.idle": "2023-09-22T18:48:29.727415Z",
     "shell.execute_reply": "2023-09-22T18:48:29.727008Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.636323Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_model = Xformer(\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    h,\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    n,\n",
    "    dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93959d3-4c98-4256-b29b-a58f837b6909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T23:57:58.237341Z",
     "iopub.status.busy": "2023-09-21T23:57:58.236505Z",
     "iopub.status.idle": "2023-09-21T23:57:58.824945Z",
     "shell.execute_reply": "2023-09-21T23:57:58.824389Z",
     "shell.execute_reply.started": "2023-09-21T23:57:58.237303Z"
    },
    "tags": []
   },
   "source": [
    "Next, we write our own training loop, taking advantage of the loss and accuracy functions we coded earlier.  \n",
    "**Note:** The default execution mode in TensorFlow 2 is *eager execution*. However, for a fairly large model such as this, we want to leverage the optimizations provided by *graph execution* (at the cost of some overhead). In order to do so, we need to use the `@function` decorator below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80371185-240c-46eb-9fdb-1594a25eb679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.728071Z",
     "iopub.status.busy": "2023-09-22T18:48:29.727957Z",
     "iopub.status.idle": "2023-09-22T18:48:29.731546Z",
     "shell.execute_reply": "2023-09-22T18:48:29.730983Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.728062Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute the training loss\n",
    "        loss = loss_fn(decoder_output, prediction)\n",
    "\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fn(decoder_output, prediction)\n",
    "\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657a8147-830d-422c-a57c-dab5fc0219d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T18:48:29.736411Z",
     "iopub.status.busy": "2023-09-22T18:48:29.736172Z",
     "iopub.status.idle": "2023-09-22T18:50:31.279340Z",
     "shell.execute_reply": "2023-09-22T18:50:31.278978Z",
     "shell.execute_reply.started": "2023-09-22T18:48:29.736390Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss 8.2269 Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.3872 Accuracy 0.1554\n",
      "Epoch 1 Step 100 Loss 6.8584 Accuracy 0.1876\n",
      "Epoch 1: Training Loss 6.6163, Training Accuracy 0.1966\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0 Loss 5.7815 Accuracy 0.2140\n",
      "Epoch 2 Step 50 Loss 5.5019 Accuracy 0.2682\n",
      "Epoch 2 Step 100 Loss 5.3345 Accuracy 0.2718\n",
      "Epoch 2: Training Loss 5.2267, Training Accuracy 0.2740\n",
      "Total time taken: 57.24s\n"
     ]
    }
   ],
   "source": [
    "train_loss = Mean(name=\"train_loss\")\n",
    "train_accuracy = Mean(name=\"train_accuracy\")\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "    \n",
    "    start_time = time()\n",
    "\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} \"\n",
    "                + f\"Accuracy {train_accuracy.result():.4f}\"\n",
    "            )\n",
    "\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch +1}: Training Loss {train_loss.result():.4f}, \"\n",
    "        + f\"Training Accuracy {train_accuracy.result():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n",
    "        \n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

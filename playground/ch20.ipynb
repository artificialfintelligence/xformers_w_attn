{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197569a5-8d40-44d0-a059-f6b19fcdffec",
   "metadata": {},
   "source": [
    "# 20 Training the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb113c5f-7a02-429b-b32f-74f18bce72a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:44.723702Z",
     "iopub.status.busy": "2023-09-22T01:13:44.721271Z",
     "iopub.status.idle": "2023-09-22T01:13:47.026215Z",
     "shell.execute_reply": "2023-09-22T01:13:47.025904Z",
     "shell.execute_reply.started": "2023-09-22T01:13:44.723519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from time import time\n",
    "\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from numpy.random import shuffle\n",
    "from tensorflow import (\n",
    "    GradientTape,\n",
    "    TensorSpec,\n",
    "    argmax,\n",
    "    cast,\n",
    "    convert_to_tensor,\n",
    "    data,\n",
    "    equal,\n",
    "    float32,\n",
    "    function,\n",
    "    int64,\n",
    "    math,\n",
    "    reduce_sum,\n",
    "    train,\n",
    ")\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from xformer.model import Xformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a804f0-f38f-4d6a-b30b-5d3224f6a50e",
   "metadata": {},
   "source": [
    "## 20.1 Preparing the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fbc83-b353-4467-8cae-34e28439fa72",
   "metadata": {},
   "source": [
    "The dataset is already standardized and clean (no punctuation, all lowercae, etc.) and it can be downloaded from [here](https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl).  \n",
    "The class below loads the data, selects a subset of it for demonstration purposes (because it's very large), appends special `<START>` and `<EOS>` tokens to the beginning and end of the sequences, splits them based on a pre-defined ratio (the train-test split), tokenizes the input and target sequences separately and uses these to deduce the maximum sequence length and vocabulary size for the encoder and decoder respectively.  \n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a9863d-3de8-49ec-81aa-f9d72aafcd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.027299Z",
     "iopub.status.busy": "2023-09-22T01:13:47.027080Z",
     "iopub.status.idle": "2023-09-22T01:13:47.032142Z",
     "shell.execute_reply": "2023-09-22T01:13:47.031897Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.027289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = (\n",
    "            10_000  # Number of sentences to include in the dataset\n",
    "        )\n",
    "        self.train_split = 0.9  # Proportion of the data to use for training\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, \"rb\"))\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[: self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        # Note: The book uses <START> but that is no good since it will be\n",
    "        # cleaned and lowercased to \"start\" and get mixed up with the actual\n",
    "        # English word \"start\", which does appear in the training data.\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<SEQSTART> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<SEQSTART> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "\n",
    "        # Split the dataset\n",
    "        train = dataset[: int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding=\"post\")\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding=\"post\")\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "        return (\n",
    "            trainX,\n",
    "            trainY,\n",
    "            train,\n",
    "            enc_seq_length,\n",
    "            dec_seq_length,\n",
    "            enc_vocab_size,\n",
    "            dec_vocab_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bd268-716c-487f-9aea-08cb4bb0449f",
   "metadata": {},
   "source": [
    "Let's test it and take a look at some sample sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e632f979-bac7-4f3e-b602-0f679573be4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.032675Z",
     "iopub.status.busy": "2023-09-22T01:13:47.032580Z",
     "iopub.status.idle": "2023-09-22T01:13:47.417163Z",
     "shell.execute_reply": "2023-09-22T01:13:47.416714Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.032666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    train_orig,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    ") = dataset(\"data/english-german-both.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a95723-850a-4b82-9d84-92eb3a321438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.417864Z",
     "iopub.status.busy": "2023-09-22T01:13:47.417768Z",
     "iopub.status.idle": "2023-09-22T01:13:47.421809Z",
     "shell.execute_reply": "2023-09-22T01:13:47.421509Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.417855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEQSTART> its true <EOS>\n",
      "tf.Tensor([  1  15 268   2   0   0   0], shape=(7,), dtype=int64)\n",
      "\n",
      "\n",
      "<SEQSTART> es ist wahr <EOS>\n",
      "tf.Tensor([  1   7   4 335   2   0   0   0   0   0   0   0], shape=(12,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    train_orig[0, 0],\n",
    "    trainX[0, :],\n",
    "    \"\\n\",\n",
    "    train_orig[0, 1],\n",
    "    trainY[0, :],\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2e978-6ffc-4449-bd03-0b7bfe757863",
   "metadata": {},
   "source": [
    "It's a dataset of very short English and German sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a18dc2-4ff4-4a19-b63c-c05eccd6e73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.423125Z",
     "iopub.status.busy": "2023-09-22T01:13:47.423032Z",
     "iopub.status.idle": "2023-09-22T01:13:47.425822Z",
     "shell.execute_reply": "2023-09-22T01:13:47.425364Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.423114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 7\n",
      "Decoder sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder sequence length:\", enc_seq_length)\n",
    "print(\"Decoder sequence length:\", dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43088d5f-659a-4e57-a49d-ee9c04ad07eb",
   "metadata": {},
   "source": [
    "## 20.2 Applying a Padding Mask\n",
    "### (And Introducing the Loss Function and Accuracy Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c1d4b-0f0a-410f-a711-bb4f90d1905a",
   "metadata": {},
   "source": [
    "So, here's the thing: Just masking the input and target sequences was not enough. We also need to exclude the masked tokens from being used in the calculation of our loss function and our accuracy metric.  \n",
    "We will be using a sparse categorical cross-entropy loss function. Here's the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ac7011-41d4-4563-90df-37eb2dbb3b43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.427905Z",
     "iopub.status.busy": "2023-09-22T01:13:47.427696Z",
     "iopub.status.idle": "2023-09-22T01:13:47.430513Z",
     "shell.execute_reply": "2023-09-22T01:13:47.430041Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.427893Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included\n",
    "    # in the computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = (\n",
    "        sparse_categorical_crossentropy(target, prediction, from_logits=True)\n",
    "        * mask\n",
    "    )\n",
    "\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8976f-b741-4797-a6c4-805aab549eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T01:18:20.328446Z",
     "iopub.status.busy": "2023-09-15T01:18:20.327739Z",
     "iopub.status.idle": "2023-09-15T01:18:21.040814Z",
     "shell.execute_reply": "2023-09-15T01:18:21.040326Z",
     "shell.execute_reply.started": "2023-09-15T01:18:20.328416Z"
    },
    "tags": []
   },
   "source": [
    "Note that the output of the decoder is a tensor of shape `(batch_size, dec_seq_length, dec_vocab_size)` and its values represent the probabilities for each vocabulary token at each position in the output sequence. In order to compare the output to the target sequence, we will pick only the highest probability token at each position (and retrieve its corresponding token/word using `argmax`) and calculate the average accuracy (which is 0 or 1 for an individual token) over all unmasked values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0bf071-1d9e-4126-a7f4-8df58c6770bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.431423Z",
     "iopub.status.busy": "2023-09-22T01:13:47.431205Z",
     "iopub.status.idle": "2023-09-22T01:13:47.437906Z",
     "shell.execute_reply": "2023-09-22T01:13:47.437387Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.431413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(math.equal(target, 0))\n",
    "\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2) + 1)\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fc8b1-b52e-4291-9248-e869c36a1020",
   "metadata": {},
   "source": [
    "## 20.3 Training the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9db65-81fb-4edd-af0d-3f532d3ef85b",
   "metadata": {},
   "source": [
    "As always, we will use the parameters used in the AIAYN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1b4226-069f-44c2-a51d-794712e78664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.438468Z",
     "iopub.status.busy": "2023-09-22T01:13:47.438362Z",
     "iopub.status.idle": "2023-09-22T01:13:47.443762Z",
     "shell.execute_reply": "2023-09-22T01:13:47.443386Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.438459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ba734-af8f-4753-a0b2-c94d6810fa34",
   "metadata": {},
   "source": [
    "And we'll use a learning rate scheduler which was specified in the same paper as follows:  \n",
    "$$\\text { lrate }=d_{\\mathrm{model}}^{-0.5} \\cdot \\min \\left(step\\_num^{-0.5}, \\text { step_num } \\cdot \\text { warmup_steps }{ }^{-1.5}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ace1ae-1593-44c4-a3b3-16f7a463bb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.444920Z",
     "iopub.status.busy": "2023-09-22T01:13:47.444518Z",
     "iopub.status.idle": "2023-09-22T01:13:47.448233Z",
     "shell.execute_reply": "2023-09-22T01:13:47.447711Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.444906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = cast(warmup_steps, float32)\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and\n",
    "        # decreasing it thereafter\n",
    "        step_num = cast(step_num, float32)\n",
    "        arg1 = step_num**-0.5\n",
    "        arg2 = step_num * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return (self.d_model**-0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac54443-1e98-4c0f-99ba-fb8bf461522b",
   "metadata": {},
   "source": [
    "Let's prepare our batches for training and instantiate our model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11806dd2-fbed-404b-b805-30dd25dadd48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.448970Z",
     "iopub.status.busy": "2023-09-22T01:13:47.448819Z",
     "iopub.status.idle": "2023-09-22T01:13:47.454310Z",
     "shell.execute_reply": "2023-09-22T01:13:47.453943Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.448950Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f726bc-5b66-42ae-9c5f-e62aef3153fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.454996Z",
     "iopub.status.busy": "2023-09-22T01:13:47.454893Z",
     "iopub.status.idle": "2023-09-22T01:13:47.470821Z",
     "shell.execute_reply": "2023-09-22T01:13:47.470033Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.454986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e15441-f59d-46b9-95d0-8be775dbe8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.471875Z",
     "iopub.status.busy": "2023-09-22T01:13:47.471453Z",
     "iopub.status.idle": "2023-09-22T01:13:47.571668Z",
     "shell.execute_reply": "2023-09-22T01:13:47.571182Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.471857Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_model = Xformer(\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    h,\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    n,\n",
    "    dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93959d3-4c98-4256-b29b-a58f837b6909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T23:57:58.237341Z",
     "iopub.status.busy": "2023-09-21T23:57:58.236505Z",
     "iopub.status.idle": "2023-09-21T23:57:58.824945Z",
     "shell.execute_reply": "2023-09-21T23:57:58.824389Z",
     "shell.execute_reply.started": "2023-09-21T23:57:58.237303Z"
    },
    "tags": []
   },
   "source": [
    "Next, we write our own training loop, taking advantage of the loss and accuracy functions we coded earlier.  \n",
    "**Note:** The default execution mode in TensorFlow 2 is *eager execution*. However, for a fairly large model such as this, we want to leverage the optimizations provided by *graph execution* (at the cost of some overhead). In order to do so, we need to use the `@function` decorator below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80371185-240c-46eb-9fdb-1594a25eb679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.572390Z",
     "iopub.status.busy": "2023-09-22T01:13:47.572288Z",
     "iopub.status.idle": "2023-09-22T01:13:47.576935Z",
     "shell.execute_reply": "2023-09-22T01:13:47.576185Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.572378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute the training loss\n",
    "        loss = loss_fn(decoder_output, prediction)\n",
    "\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fn(decoder_output, prediction)\n",
    "\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657a8147-830d-422c-a57c-dab5fc0219d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:13:47.580196Z",
     "iopub.status.busy": "2023-09-22T01:13:47.580041Z",
     "iopub.status.idle": "2023-09-22T01:14:04.101858Z",
     "shell.execute_reply": "2023-09-22T01:14:04.101184Z",
     "shell.execute_reply.started": "2023-09-22T01:13:47.580186Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss nan Accuracy 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m train_batchY[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m train_batchY[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = Mean(name=\"train_loss\")\n",
    "train_accuracy = Mean(name=\"train_accuracy\")\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} \"\n",
    "                + f\"Accuracy {train_accuracy.result():.4f}\"\n",
    "            )\n",
    "\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch +1}: Training Loss {train_loss.result():.4f}, \"\n",
    "        + f\"Training Accuracy {train_accuracy.result():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

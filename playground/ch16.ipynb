{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2592246-54bd-479b-bca0-0ed3c008bdd1",
   "metadata": {},
   "source": [
    "# 16 Implementing Multi-Head Attention in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f73e73f-510f-4fac-bf4a-9a45708b71ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T23:31:29.562884Z",
     "iopub.status.busy": "2023-08-27T23:31:29.560826Z",
     "iopub.status.idle": "2023-08-27T23:31:29.579118Z",
     "shell.execute_reply": "2023-08-27T23:31:29.578659Z",
     "shell.execute_reply.started": "2023-08-27T23:31:29.562809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "from tensorflow import cast, float32, math, matmul, reshape, shape, transpose\n",
    "from tensorflow.keras.backend import softmax\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc92a6b-2565-403f-bb66-c00497437096",
   "metadata": {},
   "source": [
    "# 16.1 Recap of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457e02a-f680-48bb-8baa-484fd175dfd3",
   "metadata": {},
   "source": [
    "Okay, so actually we left out some pretty important details in Ch. 15 but we'll go over them now.  \n",
    "\n",
    "First off, within each attention block there are actually multiple attention mechanisms (\"heads\") working in parallel (literally, the input is fed to them in parallel). This, in theory, allows the model to pay various \"kinds\" of attention. In the NLP context, you could think of this as allowing the model to capture the different ways in which different parts of the input sequence are related. (Grammatical, semantic, temporal, person/tense of verbs and their agreement with subjects, pronouns, et cetera).  \n",
    "\n",
    "Secondly, there are multiple \"linear projection matrices\". There is one per attention head for each of Q, K and V. Essentially these are trainable weight matrices for queries, keys and values that generate different subspace representations of them. Each attention head then works on of these projected versions of Q, K and V. There is also one right at the end which produces a projection of the concatenated outputs of all the different heads. Once again, these weights are learned during training. (You can think of each as a Dense/FF layer.  \n",
    "\n",
    "Did you catch that?! The outputs of the various scaled dot product attention heads is joined via a concatenation operation. That is the third important detail.  \n",
    "\n",
    "By the way, in the AIAYN transformer they had 8 attention heads. And one more thing that we only mentioned in passing is that the \"encoder block\" and \"decoder block\" are actually _stacks_ of architecturally identical blocks. In the AIAYN paper they had 6 of them. I guess we'll get to that eventually, when we code up the entire transformer.  \n",
    "\n",
    "**Note:** There is nothing magical about the aforementioned numbers (`6` and `8`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518684-6011-471d-8c76-2050590d4324",
   "metadata": {},
   "source": [
    "## 16.2 Implementing Multi-Head Attention from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21e8c3-69e8-4418-8110-b3a74b9a03e8",
   "metadata": {},
   "source": [
    "First, let us import our scaled dot-product attention layer from the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ea91fc-668c-44e9-b4b6-dd548fbac659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T20:12:15.989386Z",
     "iopub.status.busy": "2023-08-27T20:12:15.987886Z",
     "iopub.status.idle": "2023-08-27T20:12:16.012147Z",
     "shell.execute_reply": "2023-08-27T20:12:16.011526Z",
     "shell.execute_reply.started": "2023-08-27T20:12:15.989336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Score the queries against the keys after transposing the latter, and then scale\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(\n",
    "            cast(d_k, float32)\n",
    "        )\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += float(\"-inf\") * mask\n",
    "        # Compute the weights using a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        # Compute attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45e49f-62ca-4348-9627-2f2651447706",
   "metadata": {},
   "source": [
    "Now we proceed to define our Multi-Head Attention layer. Things are about to get very, VERY messy and confusing. Part 3 of the \"Transformers Explained Visually\" series on Towards Data Science ([here](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)) was extremely helpful in understanding what is about to come. (A word of warning: Although the images on that page help immensely in clarifying the logic, the visual representations of dimensions, i.e. the _visual_ height/width/depth of lines, are _not_ to be taken too literally. Focus on the dimension labels instead).  \n",
    "\n",
    "Here's what's about to happen: The outputs of the linear layers that produce the Q, K and V matrices, that is to say the Q, K and V matrices themselves, are going to be \"split\" between the different attention heads. But this is _not_ a \"physical\" split. It is a \"logical\" one. That is to say, each attention head is going to process logically separate sections of the _same, single_ Q (or K or V) matrix. So, in effect, all attention heads share the same linear layer, but operate on their \"own\" logical section of each data matrix. This is just so that the computations of all attention heads can be performed in a single matrix operation rather than N separate operations (vectorization/parallelization ftw). This keeps the model simple (due to fewer linear layers being needed) while achieving the power of independent attention heads.  \n",
    "\n",
    "Let's forget about the `batch_size` dimension for now (but keep it in the background of our minds!) and focus on one example input/target sequence for simplicity. The single example embedded sequence comes in to the linear layer with dimensions $(L_{seq} \\times d_{model})$, gets matrix-multiplied by the $(d_{model} \\times d_{model})$ `W_q`, `W_k` and `W_v` matrices to yield the Q, K and V matrices, still of dimensions $(L_{seq} \\times d_{model})$. Then these get reshaped. How? Let's focus on the query matrix Q (the other two follow an identical logic). If we have $h$ heads, then let $s = d_{model} \\div h$. We will first reshape our Q to have dimensions $(L_{seq} \\times h \\times s)$, then reshape it again to have dimensions $(h \\times L_{seq} \\times s)$.  \n",
    "Again, it doesn't matter why they are \"physically\" split like this. What matters are the _logical_ splits, and those are visualized quite well in the article linked to earlier.  \n",
    "\n",
    "I told you this was going to get messy! But fortunately we don't have to keep track of everything as `tf.reshape()` will take care of the grunt work for us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888d084-669c-43cf-aa70-ae24f843d2f4",
   "metadata": {},
   "source": [
    "In short: \n",
    "- We need to reshape the linearly projected queries, keys, and values so that attention heads can work in parallel.\n",
    "- Queries, keys and values come in with dimensions `(batch_size, seq_length, d_model)`.\n",
    "- They are then linearly projected to dimensions `(batch_size, seq_length, d_?)` where `?` is `q`, `k` or `v`.\n",
    "- Then they are rearranged to have dimensions `(batch_size, n_heads, seq_length, depth)` using the helper method `reshape_tensor()`. (Note: `depth` is the same thing as $s$ above and `n_heads` is just $h$).  \n",
    "  But this is done in two steps:\n",
    "  - First they're reshaped to dimensions `(batch_size, seq_length, n_heads, depth)`.\n",
    "  - Then the second and third dimensions are transposed.  \n",
    "  \n",
    "With all of that said and done, `d_k` and `d_v` below will both equal `d_model / h`. To be honest, I'm not sure why they are allowed to have static, independent values. The book I'm following doesn't have the clearest explanations, hence all the supplementary resources I study and link to...  \n",
    "\n",
    "\n",
    "Note: The `reshape_tensor()` method also has a `flag` argument that allows us to undo (revert) the operation. This is useful when \"stitching\" (concatenating) the outputs of all the attention heads back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce77153a-ac5b-4a1a-84d9-546ee59c3ec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-28T00:03:56.854301Z",
     "iopub.status.busy": "2023-08-28T00:03:56.853146Z",
     "iopub.status.idle": "2023-08-28T00:03:56.876492Z",
     "shell.execute_reply": "2023-08-28T00:03:56.874333Z",
     "shell.execute_reply.started": "2023-08-28T00:03:56.854229Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads\n",
    "        self.d_k = d_k  # Dim of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dim of the linearly projected values\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries, ...\n",
    "        self.W_k = Dense(d_k)  # ... for the keys\n",
    "        self.W_v = Dense(d_v)  # ... for the values\n",
    "        self.W_o = Dense(d_model)  # ... for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_model)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], -1))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys,\n",
    "        # and values\n",
    "        o_reshaped = self.attention(\n",
    "            q_reshaped, k_reshaped, v_reshaped, self.d_k, mask\n",
    "        )\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        \n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "        \n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039916da-4ea5-4033-a7c7-d07842419f27",
   "metadata": {},
   "source": [
    "## 16.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0d8ea-1f04-493c-af90-a93d5ccb1aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

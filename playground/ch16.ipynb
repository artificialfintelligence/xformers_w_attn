{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2592246-54bd-479b-bca0-0ed3c008bdd1",
   "metadata": {},
   "source": [
    "# 16 Implementing Multi-Head Attention in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f73e73f-510f-4fac-bf4a-9a45708b71ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T01:51:36.614643Z",
     "iopub.status.busy": "2023-09-03T01:51:36.614087Z",
     "iopub.status.idle": "2023-09-03T01:51:38.932649Z",
     "shell.execute_reply": "2023-09-03T01:51:38.932341Z",
     "shell.execute_reply.started": "2023-09-03T01:51:36.614552Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import cast, float32, math, matmul, reshape, shape, transpose\n",
    "from tensorflow.keras.backend import softmax\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc92a6b-2565-403f-bb66-c00497437096",
   "metadata": {},
   "source": [
    "# 16.1 Recap of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457e02a-f680-48bb-8baa-484fd175dfd3",
   "metadata": {},
   "source": [
    "Okay, so actually we left out some pretty important details in Ch. 15 but we'll go over them now.  \n",
    "\n",
    "First off, within each attention block there are actually multiple attention mechanisms (\"heads\") working in parallel (literally, the input is fed to them in parallel). This, in theory, allows the model to pay various \"kinds\" of attention. In the NLP context, you could think of this as allowing the model to extract various \"aspects\" or \"qualities\" (e.g. temporal, gender, cardinality, et cetera) of words in the sequence from the word embeddings during training.  \n",
    "\n",
    "Secondly, there are multiple \"linear projection matrices\". There is one per attention head for each of Q, K and V. Essentially these are trainable weight matrices for queries, keys and values that generate different subspace representations of them. Each attention head then works on of these projected versions of Q, K and V. There is also one right at the end which produces a projection of the concatenated outputs of all the different heads. Once again, these weights are learned during training. (You can think of each as a Dense/FF layer.  \n",
    "\n",
    "Did you catch that?! The outputs of the various scaled dot product attention heads is joined via a concatenation operation. That is the third important detail.  \n",
    "\n",
    "By the way, in the AIAYN transformer they had 8 attention heads. And one more thing that we only mentioned in passing is that the \"encoder block\" and \"decoder block\" are actually _stacks_ of architecturally identical blocks. In the AIAYN paper they had 6 of them. I guess we'll get to that eventually, when we code up the entire transformer.  \n",
    "\n",
    "**Note:** There is nothing magical about the aforementioned numbers (`6` and `8`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518684-6011-471d-8c76-2050590d4324",
   "metadata": {},
   "source": [
    "## 16.2 Implementing Multi-Head Attention from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21e8c3-69e8-4418-8110-b3a74b9a03e8",
   "metadata": {},
   "source": [
    "First, let us import our scaled dot-product attention layer from the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ea91fc-668c-44e9-b4b6-dd548fbac659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T01:51:38.933874Z",
     "iopub.status.busy": "2023-09-03T01:51:38.933676Z",
     "iopub.status.idle": "2023-09-03T01:51:38.936600Z",
     "shell.execute_reply": "2023-09-03T01:51:38.936329Z",
     "shell.execute_reply.started": "2023-09-03T01:51:38.933864Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        d_k = shape(keys)[-1]\n",
    "        # Score the queries against the keys after transposing the latter, and then scale\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(\n",
    "            cast(d_k, float32)\n",
    "        )\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += float(\"-inf\") * mask\n",
    "        # Compute the weights using a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        # Compute attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45e49f-62ca-4348-9627-2f2651447706",
   "metadata": {},
   "source": [
    "Now we proceed to define our Multi-Head Attention layer. Things are about to get very, VERY messy and confusing. Part 3 of the \"Transformers Explained Visually\" series on Towards Data Science ([here](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)) was extremely helpful in understanding what is about to come. (A word of warning: Although the images on that page help immensely in clarifying the logic, the visual representations of dimensions, i.e. the _visual_ height/width/depth of lines, are _not_ to be taken too literally. Focus on the dimension labels instead).  \n",
    "\n",
    "Here's what's about to happen: The outputs of the linear layers that produce the Q, K and V matrices, that is to say the Q, K and V matrices themselves, are going to be \"split\" between the different attention heads. But this is _not_ a \"physical\" split. It is a \"logical\" one. That is to say, each attention head is going to process logically separate sections of the _same, single_ Q (or K or V) matrix. So, in effect, all attention heads share the same linear layer, but operate on their \"own\" logical section of each data matrix. This is just so that the computations of all attention heads can be performed in a single matrix operation rather than N separate operations (vectorization/parallelization ftw). This keeps the model simple (due to fewer linear layers being needed) while achieving the power of independent attention heads.  \n",
    "\n",
    "Let's forget about the `batch_size` dimension for now (but keep it in the background of our minds!) and focus on one example input/target sequence for simplicity. The single example embedded sequence comes in to the linear layer with dimensions $(L_{seq} \\times d_{model})$, gets matrix-multiplied by the $(d_{model} \\times d_{model})$ `W_q`, `W_k` and `W_v` matrices to yield the Q, K and V matrices, still of dimensions $(L_{seq} \\times d_{model})$. Then these get reshaped. How? Let's focus on the query matrix Q (the other two follow an identical logic). If we have $h$ heads, then let \"head size\" $s = d_{model} \\div h$. We will first reshape our Q to have dimensions $(L_{seq} \\times h \\times s)$, then reshape it again to have dimensions $(h \\times L_{seq} \\times s)$.  \n",
    "Again, it doesn't matter why they are \"physically\" split like this. What matters are the _logical_ splits, and those are visualized quite well in the article linked to earlier.  \n",
    "\n",
    "I told you this was going to get messy! But fortunately we don't have to keep track of everything as `tf.reshape()` will take care of the grunt work for us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888d084-669c-43cf-aa70-ae24f843d2f4",
   "metadata": {},
   "source": [
    "In short: \n",
    "- We need to reshape the linearly projected queries, keys, and values so that attention heads can work in parallel.\n",
    "- Queries, keys and values come in with dimensions `(batch_size, seq_length, d_model)`.\n",
    "- They are then linearly projected to dimensions `(batch_size, seq_length, d_model)`. But since they will be 'split' between `n_heads` heads, then each head will be operating on a `(batch_size, seq_length, d_*)` slice, where `*` is `q`, `k` or `v`.\n",
    "- Each slice is internally rearranged to have dimensions `(batch_size, n_heads, seq_length, d_*)` using the helper method `reshape_tensor()`. (Note: The `d_*` are the same thing as $s$ earlier and `n_heads` is just $h$).  \n",
    "  But this is done in two steps:\n",
    "  - First they're reshaped to dimensions `(batch_size, seq_length, n_heads, d_*)`.\n",
    "  - Then the second and third dimensions are transposed.  \n",
    "  \n",
    "With all of that said and done, `d_k` and `d_v` below will both equal `d_model / h`. To be honest, I'm not sure why they are allowed to have static, independent values in the code from the book I'm following. The book doesn't have the clearest explanations (hence all the other resources I find, study and link to). I guess they were trying to conserve generality, and to be fair, the AIAYN paper does the same, but both fail to explain how this can be useful. In any case, `d_model` must, of course, be divisible by `h` and going forward I will modify the book's code and impose the AIAYN implementation where $d_k = d_v = d_{model} / h$.  \n",
    "\n",
    "**Notes**:\n",
    "1. Notice how the number of parameters for multi-head attention is the same as the number of parameters in the equivalent single-head attention. The parameters are merely divided between heads.  \n",
    "2. We could have split the _input_ matrices (i.e. the queries, keys and values themselves) between the attention heads _before_ linearly projecting them to get Q, K and V. We would have achieved the same result. But the way we've done it is more streamlined.  \n",
    "3. The `reshape_tensor()` method below also has a 'flag' argument that allows us to reuse the method to undo (revert) the operation. This is useful for \"stitching\" (concatenating) the outputs of all the attention heads back together. Set `in_flag` to `True` on the way \"in\" to multi-head attention and to `False` on your way \"out\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce77153a-ac5b-4a1a-84d9-546ee59c3ec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T01:51:38.937178Z",
     "iopub.status.busy": "2023-09-03T01:51:38.937082Z",
     "iopub.status.idle": "2023-09-03T01:51:38.941322Z",
     "shell.execute_reply": "2023-09-03T01:51:38.941085Z",
     "shell.execute_reply.started": "2023-09-03T01:51:38.937170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, n_heads, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.W_q = Dense(d_model)  # Learned projection matrix for the queries, ...\n",
    "        self.W_k = Dense(d_model)  # ... for the keys\n",
    "        self.W_v = Dense(d_model)  # ... for the values\n",
    "        self.W_o = Dense(d_model)  # ... for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, n_heads, in_flag):\n",
    "        if in_flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, n_heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], n_heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_model)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], -1))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.n_heads, True)\n",
    "        # Resulting tensor shape: (batch_size, n_heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.n_heads, True)\n",
    "        # Resulting tensor shape: (batch_size, n_heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.n_heads, True)\n",
    "        # Resulting tensor shape: (batch_size, n_heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys,\n",
    "        # and values\n",
    "        o = self.attention(q_reshaped, k_reshaped, v_reshaped, mask)\n",
    "        # Resulting tensor shape: (batch_size, n_heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        o_reshaped = self.reshape_tensor(o, self.n_heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(o_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039916da-4ea5-4033-a7c7-d07842419f27",
   "metadata": {},
   "source": [
    "## 16.3 Testing Out the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c0d8ea-1f04-493c-af90-a93d5ccb1aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T01:51:38.941905Z",
     "iopub.status.busy": "2023-09-03T01:51:38.941799Z",
     "iopub.status.idle": "2023-09-03T01:51:38.991703Z",
     "shell.execute_reply": "2023-09-03T01:51:38.991326Z",
     "shell.execute_reply.started": "2023-09-03T01:51:38.941896Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.08668005 -0.34949362 -0.22266848 ...  0.40965605  0.06909521\n",
      "    0.08180304]\n",
      "  [ 0.07395439 -0.34200007 -0.22169738 ...  0.40601394  0.05574201\n",
      "    0.09406243]\n",
      "  [ 0.08793721 -0.34833896 -0.23269483 ...  0.3852249   0.0752575\n",
      "    0.09154548]\n",
      "  [ 0.07578394 -0.347628   -0.23891406 ...  0.3904048   0.06613918\n",
      "    0.08567632]\n",
      "  [ 0.09159297 -0.3573392  -0.22204575 ...  0.39930034  0.08964753\n",
      "    0.09410493]]\n",
      "\n",
      " [[ 0.41005027 -0.48348767 -0.4015091  ...  0.02957688 -0.02041089\n",
      "    0.29514012]\n",
      "  [ 0.381046   -0.50437117 -0.3968576  ...  0.0698523  -0.00988465\n",
      "    0.2714675 ]\n",
      "  [ 0.3984901  -0.48702395 -0.4008778  ...  0.03353297 -0.01282719\n",
      "    0.26852715]\n",
      "  [ 0.40242472 -0.49919808 -0.3914452  ...  0.05083659 -0.00321215\n",
      "    0.28330266]\n",
      "  [ 0.40231907 -0.48737103 -0.39299673 ...  0.03301763 -0.00211733\n",
      "    0.28561765]]\n",
      "\n",
      " [[ 0.12036569 -0.45849591 -0.15600437 ...  0.44170716  0.00622425\n",
      "    0.10504535]\n",
      "  [ 0.12901737 -0.4721797  -0.17043318 ...  0.45872706  0.01929161\n",
      "    0.10831106]\n",
      "  [ 0.1335886  -0.4777766  -0.1718228  ...  0.45162922  0.00457408\n",
      "    0.12501526]\n",
      "  [ 0.14060156 -0.4756441  -0.15427533 ...  0.46078062  0.0058595\n",
      "    0.11109488]\n",
      "  [ 0.12311785 -0.45502976 -0.18101712 ...  0.46858773 -0.00492279\n",
      "    0.10557119]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.06686632 -0.3465086  -0.13861266 ...  0.34420127  0.03095566\n",
      "    0.09852009]\n",
      "  [ 0.0839892  -0.36212337 -0.11342894 ...  0.30218604 -0.0021081\n",
      "    0.08807366]\n",
      "  [ 0.08878449 -0.3448835  -0.1375895  ...  0.3149879   0.00848324\n",
      "    0.09749096]\n",
      "  [ 0.08661085 -0.3476814  -0.1451468  ...  0.33609945  0.01292133\n",
      "    0.08915388]\n",
      "  [ 0.08855036 -0.3500339  -0.11736539 ...  0.32119274  0.00657085\n",
      "    0.09103793]]\n",
      "\n",
      " [[-0.0196605  -0.48634467 -0.13817288 ...  0.30554053  0.10742359\n",
      "    0.12861873]\n",
      "  [-0.03179479 -0.47697723 -0.14339277 ...  0.31459883  0.11766474\n",
      "    0.14317453]\n",
      "  [ 0.00550437 -0.47788203 -0.12922572 ...  0.29580763  0.10143473\n",
      "    0.16733973]\n",
      "  [-0.00754932 -0.4868436  -0.1382972  ...  0.28574723  0.1029257\n",
      "    0.16647553]\n",
      "  [-0.00504732 -0.4914868  -0.12447212 ...  0.3090573   0.10299751\n",
      "    0.12519318]]\n",
      "\n",
      " [[-0.02498618 -0.2526967  -0.13392341 ... -0.04253936 -0.06948254\n",
      "    0.06279443]\n",
      "  [-0.04078007 -0.25647557 -0.13092525 ... -0.03645602 -0.07863178\n",
      "    0.07870453]\n",
      "  [-0.03021589 -0.26356333 -0.14790265 ... -0.04086658 -0.09095486\n",
      "    0.0712947 ]\n",
      "  [-0.00820187 -0.2631494  -0.14616224 ... -0.05682026 -0.0815859\n",
      "    0.07380354]\n",
      "  [-0.02585274 -0.268839   -0.12479791 ... -0.03843109 -0.07953254\n",
      "    0.08087476]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_model = 512  # Dimensionality of the model (the input embeddings, as well as all its sub-layers' outputs)\n",
    "batch_size = (\n",
    "    64  # Batch size from the training process; a training hyperparameter\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "queries = rng.random((batch_size, input_seq_length, d_model))\n",
    "keys = rng.random((batch_size, input_seq_length, d_model))\n",
    "values = rng.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "multihead_attention = MultiHeadAttention(h, d_model)\n",
    "output = multihead_attention(queries, keys, values)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

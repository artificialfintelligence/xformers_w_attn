{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc38205-2c16-4a2f-904f-ff4949179e04",
   "metadata": {},
   "source": [
    "# 19 Joining the Transformer Encoder and Decoder with Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a00cf-76c9-4a60-b118-ed2695a7e72b",
   "metadata": {},
   "source": [
    "Let's go over masking first and then stitch everything together (our encoder and decoder which we already implemented and will simply import)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe0c8c4-1ee2-4308-8277-6d542901f635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.287829Z",
     "iopub.status.busy": "2023-09-03T02:09:51.286837Z",
     "iopub.status.idle": "2023-09-03T02:09:51.408282Z",
     "shell.execute_reply": "2023-09-03T02:09:51.407898Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.287722Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import cast, float32, linalg, math, maximum, newaxis, ones\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from xformer.decoder import Decoder\n",
    "from xformer.encoder import Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285798f-5cb4-4a23-bfe6-7232a1d9180b",
   "metadata": {},
   "source": [
    "## 19.2 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26569a62-d266-47c6-b7b0-4952d83d8bb6",
   "metadata": {},
   "source": [
    "### Creating a Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bfa4a9-01c0-423f-bbe4-a7f9664d98a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.409563Z",
     "iopub.status.busy": "2023-09-03T02:09:51.409219Z",
     "iopub.status.idle": "2023-09-03T02:09:51.439050Z",
     "shell.execute_reply": "2023-09-03T02:09:51.438604Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.409552Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding_mask(inuput):\n",
    "    # Create mask marking zero padding values in the input by 1s\n",
    "    mask = math.equal(input, 0)\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d97b0ab-a3c8-4d2c-8435-99970ffb79c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.439921Z",
     "iopub.status.busy": "2023-09-03T02:09:51.439688Z",
     "iopub.status.idle": "2023-09-03T02:09:51.470077Z",
     "shell.execute_reply": "2023-09-03T02:09:51.469744Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.439907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  Let's test it...\n",
    "input = np.array([1, 2, 3, 4, 0, 0, 0])\n",
    "print(padding_mask(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15bc7c-329d-4b19-8138-1ca137cd80b5",
   "metadata": {},
   "source": [
    "**Note:** 1 means mask it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959eb8b-66a4-49be-8105-87e0211798d8",
   "metadata": {},
   "source": [
    "### Creating a Look-Ahead Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb3842f-5d51-4888-b883-820580504842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.470814Z",
     "iopub.status.busy": "2023-09-03T02:09:51.470684Z",
     "iopub.status.idle": "2023-09-03T02:09:51.499114Z",
     "shell.execute_reply": "2023-09-03T02:09:51.498794Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.470804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookahead_mask(n_tokens):\n",
    "    # Mask out \"future\" entries by marking them with a 1.0\n",
    "    mask = 1 - linalg.band_part(ones((n_tokens, n_tokens)), -1, 0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d42de2-7147-463f-9ee7-c7528e679627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.500521Z",
     "iopub.status.busy": "2023-09-03T02:09:51.500422Z",
     "iopub.status.idle": "2023-09-03T02:09:51.528920Z",
     "shell.execute_reply": "2023-09-03T02:09:51.528593Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.500512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's take it for a spin...\n",
    "print(lookahead_mask(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536cfe2-b6a6-43f2-9c89-e16e4e93af12",
   "metadata": {},
   "source": [
    "**Note:** Once again 1 means it's masked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fb184-bdae-4f8f-9f6a-3d9e54f404cd",
   "metadata": {},
   "source": [
    "## 19.3 Joining the Transformer Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1bb63f7-2290-4cbb-af43-053c4760fcb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.529563Z",
     "iopub.status.busy": "2023-09-03T02:09:51.529461Z",
     "iopub.status.idle": "2023-09-03T02:09:51.560723Z",
     "shell.execute_reply": "2023-09-03T02:09:51.560401Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.529554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Xformer(Model):\n",
    "    def padding_mask(self, inuput):\n",
    "        # Create mask marking zero padding values in the input by 1s\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def lookahead_mask(self, n_tokens):\n",
    "        # Mask out \"future\" entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((n_tokens, n_tokens)), -1, 0)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_vocab_size,\n",
    "        dec_vocab_size,\n",
    "        enc_seq_len,\n",
    "        dec_seq_len,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff_inner,\n",
    "        n_layers,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up the encoder block\n",
    "        self.encoder = Encoder(\n",
    "            enc_vocab_size,\n",
    "            enc_seq_len,\n",
    "            n_heads,\n",
    "            d_model,\n",
    "            d_ff_inner,\n",
    "            n_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Set up the decoder block\n",
    "        self.decoder = Decoder(\n",
    "            dec_vocab_size,\n",
    "            dec_seq_len,\n",
    "            n_heads,\n",
    "            d_model,\n",
    "            d_ff_inner,\n",
    "            n_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Define the final Dense layer that maps output probabilities to tokens in the target vocabulary\n",
    "        self.final_layer = Dense(dec_vocab_size)\n",
    "\n",
    "    def call(self, enc_input, dec_input, training):\n",
    "        # Create padding mask to mask the encoder inputs as well as the encoder outputs (which are input to the decoder)\n",
    "        enc_mask = self.padding_mask(enc_input)\n",
    "\n",
    "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "        dec_padding_mask = self.padding_mask(dec_input)\n",
    "        dec_lookahead_mask = self.lookahead_mask(dec_input.shape[1])\n",
    "        dec_mask = maximum(dec_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "        # Feed inputs into the encoder\n",
    "        enc_output = self.encoder(enc_input, enc_padding_mask, training)\n",
    "\n",
    "        # Feed encoder output into the decoder\n",
    "        dec_output = self.decoder(\n",
    "            dec_input, dec_mask, enc_output, enc_mask, training\n",
    "        )\n",
    "\n",
    "        # Pass decoder output through a final Dense layer\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70fa81-c95b-4db4-b236-531cb4432eaf",
   "metadata": {},
   "source": [
    "## 19.4 Creating an Instance of the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8e438-1b46-4489-b923-25ef0f704709",
   "metadata": {},
   "source": [
    "As usual, we will stick to the parameter values used in AIAYN and dummy input values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8f0982-a138-4284-bcdd-e014a9223ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.561333Z",
     "iopub.status.busy": "2023-09-03T02:09:51.561241Z",
     "iopub.status.idle": "2023-09-03T02:09:51.667018Z",
     "shell.execute_reply": "2023-09-03T02:09:51.666415Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.561325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = 8  # Number of attention heads\n",
    "d_ff = 2048  # Dimensionality of the inner fully-connected layer\n",
    "d_model = 512  # Dimensionality of the model\n",
    "n = 6  # Number of layers in the encoder and decoder stacks\n",
    "\n",
    "dropout_rate = 0.1  # Frequency of dropping input units in dropout layers\n",
    "\n",
    "enc_vocab_size = 20  # Vocabulary size for the encoder\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 5  # Maximum length of the input sequence\n",
    "dec_seq_length = 5  # Maximum length of the target sequence\n",
    "\n",
    "training_model = Xformer(\n",
    "    enc_vocab_size,\n",
    "    dec_vocab_size,\n",
    "    enc_seq_length,\n",
    "    dec_seq_length,\n",
    "    h,\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    n,\n",
    "    dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb93bf4-4e67-4890-99bc-106e987cbaa0",
   "metadata": {},
   "source": [
    "### Printing Out a Summary of the Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5809ee5-4c44-401f-a401-f3bd3589f7c5",
   "metadata": {},
   "source": [
    "In order to do this, we need to add a few lines of code to our `EncoderLayer` and `DecoderLayer` classes, which I have done retroactively (and added the necessary imports and method arguments to make it all work).  \n",
    "\n",
    "In the `__init__()` methods of both, we add:  \n",
    "```python\n",
    "self.build(input_shape=[None, sequence_length, d_model])\n",
    "```\n",
    "\n",
    "In the `EncoderLayer` class we add the following method:\n",
    "```python\n",
    "def build_graph(self):\n",
    "    input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "    return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "```\n",
    "\n",
    "And in `DecoderLayer` we add:\n",
    "```python\n",
    "def build_graph(self):\n",
    "    input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "    return Model(inputs=[input_layer], outputs=self.call(input_layer, None, input_layer, None, True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78406d0b-ab1d-4ce1-a34f-656e994946d7",
   "metadata": {},
   "source": [
    "Now we can build a single `EncoderLayer` or `DecoderLayer` as a `Model` and look at its summary (number of parameters, input/output shapes, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea7ffb9-ca63-4d9a-8515-40a4302c27ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-03T02:09:51.667723Z",
     "iopub.status.busy": "2023-09-03T02:09:51.667609Z",
     "iopub.status.idle": "2023-09-03T02:09:52.160750Z",
     "shell.execute_reply": "2023-09-03T02:09:52.160474Z",
     "shell.execute_reply.started": "2023-09-03T02:09:51.667714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 5, 512)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (M  (None, 5, 512)               1050624   ['input_2[0][0]',             \n",
      " ultiHeadAttention)                                                  'input_2[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)        (None, 5, 512)               0         ['multi_head_attention_37[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_and_norm_62 (AddAndNor  (None, 5, 512)               1024      ['input_2[0][0]',             \n",
      " m)                                                                  'dropout_66[0][0]']          \n",
      "                                                                                                  \n",
      " feed_forward_25 (FeedForwa  (None, 5, 512)               2099712   ['add_and_norm_62[0][0]']     \n",
      " rd)                                                                                              \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)        (None, 5, 512)               0         ['feed_forward_25[0][0]']     \n",
      "                                                                                                  \n",
      " add_and_norm_63 (AddAndNor  (None, 5, 512)               1024      ['add_and_norm_62[0][0]',     \n",
      " m)                                                                  'dropout_67[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3152384 (12.03 MB)\n",
      "Trainable params: 3152384 (12.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 5, 512)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (M  (None, 5, 512)               1050624   ['input_3[0][0]',             \n",
      " ultiHeadAttention)                                                  'input_3[0][0]',             \n",
      "                                                                     'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)        (None, 5, 512)               0         ['multi_head_attention_38[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_and_norm_64 (AddAndNor  (None, 5, 512)               1024      ['input_3[0][0]',             \n",
      " m)                                                                  'dropout_68[0][0]']          \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (M  (None, 5, 512)               1050624   ['add_and_norm_64[0][0]',     \n",
      " ultiHeadAttention)                                                  'input_3[0][0]',             \n",
      "                                                                     'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)        (None, 5, 512)               0         ['multi_head_attention_39[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_and_norm_65 (AddAndNor  (None, 5, 512)               1024      ['add_and_norm_64[0][0]',     \n",
      " m)                                                                  'dropout_69[0][0]']          \n",
      "                                                                                                  \n",
      " feed_forward_26 (FeedForwa  (None, 5, 512)               2099712   ['add_and_norm_65[0][0]']     \n",
      " rd)                                                                                              \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 5, 512)               0         ['feed_forward_26[0][0]']     \n",
      "                                                                                                  \n",
      " add_and_norm_66 (AddAndNor  (None, 5, 512)               1024      ['add_and_norm_65[0][0]',     \n",
      " m)                                                                  'dropout_70[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4204032 (16.04 MB)\n",
      "Trainable params: 4204032 (16.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from xformer.encoder import EncoderLayer\n",
    "from xformer.decoder import DecoderLayer\n",
    "\n",
    "enc_sub_layer = EncoderLayer(enc_seq_length, h, d_model, d_ff, dropout_rate)\n",
    "enc_sub_layer.build_graph().summary()\n",
    "\n",
    "dec_sub_layer = DecoderLayer(dec_seq_length, h, d_model, d_ff, dropout_rate)\n",
    "dec_sub_layer.build_graph().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
